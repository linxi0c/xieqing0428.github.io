<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[BigData复习笔记03：推荐算法]]></title>
    <url>%2Fposts%2Fc6d1abe8%2F</url>
    <content type="text"><![CDATA[推荐系统![kari-shea-Pe97ltx47Lw-unsplash](https://image.alessa0.cn/120137.jpg)Chapter03. 推荐算法推荐系统个性化推荐系统，基于海量数据向用户建议商品基于数据，利用算法，对用户和商品深度挖掘常用推荐算法基于人口的推荐最容易实现根据用户基本信息发现用户之间的相关程度，将相似用户喜欢的商品推荐给当前用户。基本信息包括：（不包含用户行为信息）性别年龄地域……基于内容的推荐Content Based，初期最为广泛使用根据商品的元信息，利用内容相关性，基于用户以往的喜好，给用户推荐相似的商品引入Item属性的CB推荐对当前Item进行内容分析，利用属性索引，在数据库中进行相关性计算，取出相关Item推荐给User优点提升推荐结果相关性结果可解释推荐结果易被用户感知缺点无个性化依赖对Item的深度分析引入User属性的CB推荐对当前Item进行内容分析，利用属性索引，结合User行为数据，在数据库中进行相关性计算，取出相关Item推荐给User优点用户模型刻画了用户兴趣需求推荐形式多样，具有个性化结果可解释缺点推荐精度低马太效应解决：试探，加入随机推荐位用户行为稀疏导致覆盖率低不同场景选取不同推荐方式【Demo】倒排表对Item进行挖掘，得出Token -&gt; ItemA:ScoreA, ItemB:ScoreB, …倒排表对Item中文分词，得出正排表fenci.py123456789101112131415161718192021import osimport sysos.system('tar xvzf jieba.tar.gz &gt; /dev/null')sys.path.append("./")import jiebaimport jieba.possegimport jieba.analysefor line in sys.stdin: ss = line.strip().split('\t') if len(ss) != 2: continue music_id, music_name = ss tmp_list = [] for x, w in jieba.analyse.extract_tags(music_name, withWeight=True): tmp_list.append((x, float(w))) final_list = sorted(tmp_list, key=lambda x: x[1], reverse=True) print('\t'.join([music_id, music_name, ','.join([':'.join([t_w[0], str(t_w[1])]) for t_w in final_list])]))转置map_inverted.py123456789101112131415import sysfor line in sys.stdin: ss = line.strip().split('\t') if len(ss) != 3: continue music_id = ss[0].strip() music_name = ss[1].strip() music_token_list = ss[2].strip() for feature in music_token_list.split(','): ss = feature.strip().split(':') token = ss[0].strip() weight = ss[1].strip() print('\t'.join([token, music_name, weight]))得到倒排表red_inverted.py12345678910111213141516171819202122232425import syscur_token = Nonem_list = []for line in sys.stdin: ss = line.strip().split('\t') if len(ss) != 3: continue token = ss[0].strip() name = ss[1].strip() weight = float(ss[2].strip()) if cur_token is None: cur_token = token if cur_token != token: final_list = sorted(m_list, key=lambda x: x[1], reverse=True) print('\t'.join([cur_token, ','.join([':'.join([name_weight[0], str(name_weight[1])]) for name_weight in final_list])])) cur_token = token m_list = [] m_list.append((name, weight))final_list = sorted(m_list, key=lambda x: x[1], reverse=True)print('\t'.join([cur_token.strip(), ','.join([':'.join([name_weight[0], str(name_weight[1])]) for name_weight in final_list])]))【Hadoop Streaming脚本】12345678910111213141516171819202122232425262728293031323334353637#!/usr/bin/env bashHADOOP_CMD="/usr/local/src/hadoop-2.8.5/bin/hadoop"STREAM_JAR_PATH="/usr/local/src/hadoop-2.8.5/share/hadoop/tools/lib/hadoop-streaming-2.8.5.jar"INPUT_FILE_PATH="/chapter03/01_inverted/music_meta.txt.small"OUTPUT_PATH_FENCI="/chapter03/01_inverted/output/python3/fenci"OUTPUT_PATH="/chapter03/01_inverted/output/python3/result"LOCAL_FILE_PATH="/mnt/hgfs/Code/chapter03/01_inverted/music_meta.txt.small"UPLOAD_PATH="/chapter03/01_inverted"$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;INPUT_FILE_PATH&#125;$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;OUTPUT_PATH&#125; $&#123;OUTPUT_PATH_FENCI&#125;$&#123;HADOOP_CMD&#125; fs -mkdir -p $&#123;UPLOAD_PATH&#125;$&#123;HADOOP_CMD&#125; fs -put $&#123;LOCAL_FILE_PATH&#125; $&#123;UPLOAD_PATH&#125;# Step 1.$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -D mapreduce.job.reduces=0 \ -D mapreduce.map.memory.mb=4096 \ -D mapreduce.job.name=jieba_fenci_demo \ -files map.py,jieba.tar.gz \ -input $&#123;INPUT_FILE_PATH&#125; \ -output $&#123;OUTPUT_PATH_FENCI&#125; \ -mapper "python map.py" \# Step 2.$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -D mapreduce.job.reduces=2 \ -D mapreduce.job.name=demo_inverted \ -files map_inverted.py,red_inverted.py \ -input $&#123;OUTPUT_PATH_FENCI&#125; \ -output $&#123;OUTPUT_PATH&#125; \ -mapper "python map_inverted.py" \ -reducer "python red_inverted.py" \基于协同的推荐从User行为日志中挖掘，User与Item关联构成UI矩阵 or IU矩阵可使用UI * IU得到UU矩阵，使用IU * UI得到II矩阵优点充分利用群体智慧推荐精度高于CB利于挖掘隐含的相关性缺点推荐结果解释性较差对时效性强的Item不适用冷启动问题User-Based CF假设：用户喜欢那些跟他有相似爱好的用户喜欢的东西具有相似兴趣的用户在未来也具有相似兴趣Item-Based CF假设：用户喜欢跟他过去喜欢的物品相似的物品历史上相似的物品未来也相似User-BasedItem-Based性能适用于用户较少场合，如果用户多，计算UU矩阵代价太大适用于物品数明显小于用户数的场合，物品多。计算II矩阵代价太大领域时效性强，用户个性化兴趣不太明显的领域长尾物品丰富，用户个性化需求强烈的领域实时性用户有新行为，不一定造成推荐结果立即变化用户有新行为，一定导致推荐结果实时变化冷启动在新用户对很少物品产生行为后，不能立即进行个性化推荐，UU矩阵需要更新。新物品上线后，一旦有用户对物品产生行为，就可以将新物品推荐给兴趣相似的其他用户新用户只要对一个物品产生行为，就可以给他推荐和该物品相关的其他物品。在没有更新II矩阵的情况下无法将新物品推荐给用户推荐理由很难提供令用户信服的推荐解释利用用户历史行为给用户做推荐解释，可以令用户比较信服协同推荐实现方案（以Item-Based为例）倒排式相似度计算公式$$ \cos (\theta)=\frac{\sum_{k=1}^{n} x_{1 k} x_{2 k}}{\sqrt{\sum_{k=1}^{n} x_{1 k}^{2}} \sqrt{\sum_{k=1}^{n} x_{2 k}^{2}}} $$处理数据格式{user, item, score}构建II矩阵归一化矩阵转置UI矩阵 -&gt; IU矩阵1_gen_ui_map.py12345678import sysfor line in sys.stdin: ss = line.strip().split('\t') if len(ss) != 3: continue user, item, score = ss print('%s\t%s\t%s' % (item, user, score))归一化1_gen_ui_red.py12345678910111213141516171819202122232425262728293031import sysimport mathcur_item = Noneuser_score_list = []for line in sys.stdin: item, user, score = line.strip().split('\t') if cur_item is None: cur_item = item if cur_item != item: sum = 0.0 for kv in user_score_list: (u, s) = kv sum += pow(s, 2) sum = math.sqrt(sum) for kv in user_score_list: (u, s) = kv print('%s\t%s\t%s' % (u, cur_item, float(s / sum))) user_score_list = [] cur_item = item user_score_list.append((user, float(score)))sum = 0.0for kv in user_score_list: (u, s) = kv sum += pow(s, 2)sum = math.sqrt(sum)for kv in user_score_list: (u, s) = kv print('%s\t%s\t%s' % (u, cur_item, float(s / sum)))生成II-pair对标准输入输出2_gen_ii_pair_map.py12345import sysfor line in sys.stdin: u, i, s = line.strip().split('\t') print("%s\t%s\t%s" % (u, i, s))生成{item, item, score}2_gen_ii_pair_red.py1234567891011121314151617181920212223242526import syscur_user = Noneitem_score_list = []for line in sys.stdin: user, item, score = line.strip().split('\t') if cur_user is None: cur_user = user if cur_user != user: for i in range(1, len(item_score_list) - 1): for j in range(i + 1, len(item_score_list)): item_a, score_a = item_score_list[i] item_b, score_b = item_score_list[j] print('%s\t%s\t%s' % (item_a, item_b, score_a * score_b)) print('%s\t%s\t%s' % (item_b, item_a, score_a * score_b)) item_score_list = [] cur_user = user item_score_list.append((item, float(score)))for i in range(1, len(item_score_list) - 1): for j in range(i + 1, len(item_score_list)): item_a, score_a = item_score_list[i] item_b, score_b = item_score_list[j] print('%s\t%s\t%s' % (item_a, item_b, score_a * score_b)) print('%s\t%s\t%s' % (item_b, item_a, score_a * score_b))求和以ii为key做partition3_sum_map.py12345import sysfor line in sys.stdin: item_a, item_b, score = line.strip().split('\t') print("%s\t%s" % (item_a + "," + item_b, score))求和3_sum_red.py1234567891011121314151617181920212223import syscur_ii_pair = Nonesum = 0.0for line in sys.stdin: ii_pair, score = line.strip().split('\t') if cur_ii_pair is None: cur_ii_pair = ii_pair if cur_ii_pair != ii_pair: ii = ii_pair.strip().split(',') if len(ii) != 2: continue item_a, item_b = ii print('%s\t%s\t%s' % (item_a, item_b, sum)) cur_ii_pair = ii_pair sum = 0.0 sum += float(score)ii = cur_ii_pair.strip().split(',')if len(ii) != 2: sys.exit()item_a, item_b = iiprint('%s\t%s\t%s' % (item_a, item_b, sum))【Hadoop Streaming脚本】1234567891011121314151617181920212223242526272829303132333435363738394041424344#!/usr/bin/env bashHADOOP_CMD="/usr/local/src/hadoop-2.8.5/bin/hadoop"STREAM_JAR_PATH="/usr/local/src/hadoop-2.8.5/share/hadoop/tools/lib/hadoop-streaming-2.8.5.jar"INPUT_FILE_PATH="/chapter03/02_cf/music_uis.data"OUTPUT_PATH_1="/chapter03/02_cf/item_based/output/python3/step1"OUTPUT_PATH_2="/chapter03/02_cf/item_based/output/python3/step2"OUTPUT_PATH_3="/chapter03/02_cf/item_based/output/python3/step3"LOCAL_FILE_PATH="/mnt/hgfs/Code/chapter03/02_cf/music_uis.data"UPLOAD_PATH="/chapter03/02_cf"$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;INPUT_FILE_PATH&#125;$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;OUTPUT_PATH_1&#125; $&#123;OUTPUT_PATH_2&#125; $&#123;OUTPUT_PATH_3&#125;$&#123;HADOOP_CMD&#125; fs -mkdir -p $&#123;UPLOAD_PATH&#125;$&#123;HADOOP_CMD&#125; fs -put $&#123;LOCAL_FILE_PATH&#125; $&#123;UPLOAD_PATH&#125;# Step 1.$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -D mapreduce.job.name=step1 \ -files 1_gen_ui_map.py,1_gen_ui_red.py \ -input $&#123;INPUT_FILE_PATH&#125; \ -output $&#123;OUTPUT_PATH_1&#125; \ -mapper "python 1_gen_ui_map.py" \ -reducer "python 1_gen_ui_red.py"# Step 2.$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -D mapreduce.job.name=step2 \ -files 2_gen_ii_pair_map.py,2_gen_ii_pair_red.py \ -input $&#123;OUTPUT_PATH_1&#125; \ -output $&#123;OUTPUT_PATH_2&#125; \ -mapper "python 2_gen_ii_pair_map.py" \ -reducer "python 2_gen_ii_pair_red.py" \# Step 3.$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -D mapreduce.job.name=step3 \ -files 3_sum_map.py,3_sum_red.py \ -input $&#123;OUTPUT_PATH_2&#125; \ -output $&#123;OUTPUT_PATH_3&#125; \ -mapper "python 3_sum_map.py" \ -reducer "python 3_sum_red.py" \冷启动问题用户冷启动提供热门排行榜，等用户数据收集到一定程度再切换到个性化推荐利用用户注册时提供的年龄、性别等数据做粗粒度的个性化利用用户社交网络账号，导入用户在社交网站上的好友信息，然后给用户推荐其好友喜欢的物品在用户新登录时要求其对一些物品进行反馈，收集这些兴趣信息，然后给用户推荐相似的物品物品冷启动把新物品推荐给可能对他感兴趣的用户，利用内容信息，将物品推荐给喜欢过和与其相似的物品的用户物品必须能够在第一时间展现给用户，否则一段时间过后，物品的价值就大大降低了User-based和Item-based都行不通时，只能利用Content-Based解决该问题，频繁更新相关性数据系统冷启动引入专家知识，通过一定的高效方式迅速建立起物品的相关性矩阵推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on Yarn用人工智能为友情链接升级换代安可推荐系统开发笔记（1）]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>复习笔记</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>推荐</tag>
        <tag>Content-Based</tag>
        <tag>协同</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BigData复习笔记02：TFIDF, LCS与中文分词]]></title>
    <url>%2Fposts%2F944875b5%2F</url>
    <content type="text"><![CDATA[TF-IDF, LCS, HMM![ole-witt-vp7gEXuon08-unsplash](https://image.alessa0.cn/122942.jpg)Chapter02. 中文分词NLP文本相似度文本相似度语义相似（从人的角度理解语句含义）字面相似（中文分词）余弦相似度/向量空间模型相似度度量计算个体间相似程度余弦相似度一个向量空间中两个向量夹角的余弦值作为衡量两个个体之间差异的大小余弦值接近1，夹角趋于0，表明两个向量越相似$$\begin{aligned} \cos (\theta) &amp;=\frac{\sum_{i=1}^{n}\left(x_{i} \times y_{i}\right)}{\sqrt{\sum_{i=1}^{n}\left(x_{i}\right)^{2}} \times \sqrt{\sum_{i=1}^{n}\left(y_{i}\right)^{2}}} \ &amp;=\frac{a \cdot b}{ | a| | x| | b| |} \end{aligned}$$案例：步骤0句子A: 这只皮鞋号码大了，那只号码合适句子B: 这只皮鞋号码不小，那只更合适步骤1: 分词句子A: 这只/皮鞋/号码/大了，那只/号码/合适句子B: 这只/皮鞋/号码/不/小，那只/更/合适步骤2: 列出所有词这只, 皮鞋, 号码, 大了, 那只, 合适, 不, 小, 更步骤3: 计算词频句子A: 这只1, 皮鞋1, 号码2, 大了1, 那只1, 合适1, 不0, 小0, 更0句子B: 这只1, 皮鞋1, 号码1, 大了0, 那只1, 合适1, 不1, 小1, 更1步骤4: 词频向量化句子A: (1, 1, 2, 1, 1, 1, 0, 0, 0)句子B: (1, 1, 1, 0, 1, 1, 1, 1, 1)步骤5: 套公式计算$$\begin{array}{l}{\cos (\theta)=\frac{1 \times 1+1 \times 1+2 \times 1+1 \times 0+1 \times 1+1 \times 1+0 \times 1+0 \times 1+0 \times 1}{\sqrt{1^{2}+1^{2}+2^{2}+1^{2}+1^{2}+1^{2}+0^{2}+0^{2}+0^{2}+1^{2}+1^{2}+1^{2}+0^{2}+1^{2}+1^{2}+1^{2}+1^{2}+1^{2}}}} \ {=\frac{6}{\sqrt{7} \times \sqrt{8}}} \ {=0.81}\end{array}$$文本相似度计算的处理流程【依据】词袋模型B.O.W找出两篇文章的关键词每篇文章各取出若干个关键词，合并成一个集合，计算每篇文章对于这个集合中的词的词频生成两篇文章各自的词频向量计算两个向量的余弦相似度，值越大越相似TD-IDF词频TF假设：如果一个词很重要，应该会在文章中多次出现词频：一个词在文章中出现的次数停用词：类似“的”, “地”, “得” 对结果毫无帮助，必须过滤掉反文档频率IDF假设：如果某个词比较少见，但是它在这篇文章中多次出现，那么它很可能反应了这篇文章的特性，正是我们所需要的关键词在词频的基础上，赋予每一个词的权重，进一步体现该词的重要性最少见的词（“的”、“是”、“在”）给予最小的权重较常见的词（“国内”、“中国”、“报道”）给予较小的权重较少见的词（“养殖”、“维基”、”涨停“）将TF和IDF进行相乘，就得到了一个词的TF-IDF值，某个词对文章重要性越高，该值越大，于是排在前面的几个词，就是这篇文章的关键词。计算步骤【前提】准备语料库计算TF：某个词在文章中出现的次数词频标准化（任选一种）TF = 某个词在文章中出现的次数 / 文章的总次数TF = 某个词在文章中出现的次数 / 该文章出现次数最多的词的出现次数计算IDF标准化IDF = log( 语料库的文档总数 / ( 包含该词的文档数 + 1 ) )计算TF-IDFTF-IDF = 词频（TF）* 反文档频率（IDF）TF-IDF与一个词在文档中的出现次数成正比，与包含该词的文档数成反比【Demo】判断相似文章步骤1: 使用TF-IDF算法，找出两篇文章的关键词步骤2: 每篇文章各取出若干个关键词，合并成一个集合，计算每篇文章对于这个集合中的词的词频（为了避免文章长度的差异，可以使用相对词频）步骤3: 生成两篇文章各自的词频向量步骤4: 计算两个向量的余弦相似度，值越大越相似【Demo】自动摘要文章的信息都包含在句子中，有些句子包含的信息多，有些句子包含的信息少。“自动摘要”就是要找出那些包含信息最多的句子。句子的信息量用“关键词”来衡量。如果包含的关键词越多，就说明这个句子越重要。只要关键词之间的距离小于“门槛值”，它们就被认为处于同一个簇之中，如果两个关键词之间有超过“门槛值”个数以上的其他词，就可以把这两个关键词分在两个簇。下一步，对于每个簇，都计算它的重要性分值。簇的重要性 = （包含的关键词数量）^2 / 簇的长度简化：不再区分“簇”，只考虑句子包含的关键词确定关键词集合取有限的次数（如Top-10）如：按TF-IDF分数 &gt; 0.7截断找出包含关键词的句子把句子做排序，对每个句子划分等级包含关键词越多越重要关键词分越高越重要把等级高的句子取出来，形成摘要优点简单快速，结果比较符合实际情况缺点单纯以“词频”做衡量标准，不够全面，有时重要的词可能出现的次数并不多该算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的解决方法：对全文的第一段和每一段的第一句话给予较大权重。【案例代码】TF-IDFTFMapmap_tf.py123456789101112131415161718import syscnt = 0for line in sys.stdin: ss = line.strip().split('\t') if len(ss) != 2: continue file_name, file_content = ss word_list = file_content.strip().split(' ') for word in word_list: cnt += 1 file_count = file_name + ':' + str(cnt) for word in word_list: print('%s\t%s\t%s' % (file_count, word, 1)) cnt = 0Reducered_tf.py123456789101112131415161718192021222324252627import sysimport mathcurrent_file = Nonecurrent_word = Nonecurrent_count = 0sum = 0for line in sys.stdin: ss = line.strip().split('\t') if len(ss) != 3: continue file_count, word, val = ss file_name, count = file_count.strip().split(':') if current_word is None: current_word = word if current_word != word: tf = math.log(float(sum) / float(current_count)) print('%s\t%s\t%s' % (current_file, current_word, tf)) current_word = word sum = 0 sum += int(val) current_file = file_name current_count = counttf = math.log(float(sum) / float(current_count))print('%s\t%s\t%s' % (current_file, current_word, tf))IDFMapmap_idf.py1234567891011121314import sysfor line in sys.stdin: ss = line.strip().split('\t') if len(ss) != 2: continue file_name, file_content = ss word_list = file_content.strip().split(' ') word_set = set(word_list) for word in word_set: print('\t'.join([word, '1']))Reducered_idf.py12345678910111213141516171819202122232425import sysimport mathcurrent_word = Nonesum = 0docs_cnt = 508for line in sys.stdin: ss = line.strip().split('\t') if len(ss) != 2: continue word, val = ss if current_word is None: current_word = word if current_word != word: idf = math.log(float(docs_cnt) / (float(sum) + 1.0)) print('\t'.join([current_word, str(idf)])) current_word = word sum = 0 sum += int(val)idf = math.log(float(docs_cnt) / (float(sum) + 1.0))print('\t'.join([current_word, str(idf)]))TF-IDFMapmap.py1234567891011121314151617181920212223242526272829303132333435363738#!/usr/bin/pythonimport sysdef read_local_file_func(file): word_map = &#123;&#125; file_in = open(file, 'r') for line in file_in: ss = line.strip().split('\t') if len(ss) != 2: continue word, idf = ss word_map[word] = idf return word_mapdef mapper_func(white_list_fd): word_map = read_local_file_func(white_list_fd) for line in sys.stdin: ss = line.strip().split('\t') if len(ss) != 3: continue file, word, tf = ss if word != "" and (word in word_map.keys()): idf = word_map[word] tfidf = float(float(tf) + float(idf)) print("%s\t%s\t%s" % (file, tfidf, word))if __name__ == "__main__": module = sys.modules[__name__] func = getattr(module, sys.argv[1]) args = None if len(sys.argv) &gt; 1: args = sys.argv[2:] func(*args)Reducered.py1234567891011121314151617181920import syscurrent_file = Noneresult = ''for line in sys.stdin: ss = line.strip().split('\t') if len(ss) != 3: continue file, tfidf, word = ss if current_file is None: current_file = file result = current_file + '\t' if current_file != file: print(result[:-1]) current_file = file result = current_file + '\t' result += word + ':' + tfidf + ' 'print(result[:-1])【Hadoop Streaming脚本】12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#!/usr/bin/env bashHADOOP_CMD="/usr/local/src/hadoop-2.8.5/bin/hadoop"STREAM_JAR_PATH="/usr/local/src/hadoop-2.8.5/share/hadoop/tools/lib/hadoop-streaming-2.8.5.jar"INPUT_FILE_PATH="/chapter02/01_tfidf/tfidf_input.data"OUTPUT_PATH_TF="/chapter02/01_tfidf/output/python3/tf"OUTPUT_PATH_IDF="/chapter02/01_tfidf/output/python3/idf"OUTPUT_PATH_TFIDF="/chapter02/01_tfidf/output/python3/tfidf"LOCAL_FILE_PATH="/mnt/hgfs/Code/chapter02/01_tfidf/tfidf_input.data"UPLOAD_PATH="/chapter02/01_tfidf"$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;INPUT_FILE_PATH&#125;$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;OUTPUT_PATH_TF&#125;$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;OUTPUT_PATH_IDF&#125;$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;OUTPUT_PATH_TFIDF&#125;$&#123;HADOOP_CMD&#125; fs -mkdir -p $&#123;UPLOAD_PATH&#125;$&#123;HADOOP_CMD&#125; fs -put $&#123;LOCAL_FILE_PATH&#125; $&#123;UPLOAD_PATH&#125;# Step 1. tf$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -D stream.num.map.output.key.fields=2 \ -D num.key.fields.for.partition=1 \ -files map_tf.py,red_tf.py \ -input $&#123;INPUT_FILE_PATH&#125; \ -output $&#123;OUTPUT_PATH_TF&#125; \ -mapper "python map_tf.py" \ -reducer "python red_tf.py" \ -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \# Step 2. idf$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -files map_idf.py,red_idf.py \ -input $&#123;INPUT_FILE_PATH&#125; \ -output $&#123;OUTPUT_PATH_IDF&#125; \ -mapper "python map_idf.py" \ -reducer "python red_idf.py" \# Step 3. tf-idf$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -D stream.num.map.output.key.fields=2 \ -D num.key.fields.for.partition=1 \ -files map.py,red.py,"hdfs://master:9000/chapter02/01_tfidf/output/python3/idf/part-00000#WH" \ -input $&#123;OUTPUT_PATH_TF&#125; \ -output $&#123;OUTPUT_PATH_TFIDF&#125; \ -mapper "python map.py mapper_func WH" \ -reducer "python red.py" \ -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \LCSLCS：最长公共子序列(Longest Common Subsequence)注意区别最长公共子串(Longest Common Substring)最长公共子串要求连续用于描述两段文字之间的“相似度”辨别抄袭，对一段文字进行修改后，计算改动前后文字的最长公共子序列。将除此子序列外的部分提取出来，用该方法判断修改的部分可用于推荐结果过滤（相似项目）解法1：暴力穷举假设字符串X, Y长度分别为m, nX的一个子序列，即下标序列{1, 2, …, m}严格递增子序列X共有2^m个不同子序列Y共有2^n个不同子序列时间复杂度: O(2^m * 2^n), 基本不可用解法2：动态规划DP$$L C S\left(X_{m}, Y_{n}\right)=\left{\begin{array}{ll}{L C S\left(X_{m-1}, Y_{n-1}\right)+x_{m}} &amp; {x_{m}=y_{n}} \ {\max \left{L C S\left(X_{m-1}, Y_{n}\right), L C S\left(X_{m}, Y_{n-1}\right)\right}} &amp; {x_{m} \neq y_{n}}\end{array}\right.$$使用二维数组C[m, n]C[i, j]记录序列Xi和Yj的最长公共子序列的长度当i = 0 或j = 0时，空序列时Xi和Yj的最长公共子序列，故C[i, j] = 0$$c(i, j)=\left{\begin{array}{c}{0} &amp; {i = 0\ or \ j = 0} \ {c(i-1, j-1)+1} &amp; {i &gt; 0,j &gt; 0\ and\ x_{i}=y_{j}} \ {\max {c(i-1, j), c(i, j-1)}} &amp; {i &gt; 0,j &gt; 0\ and\ x_{i} \neq y_{j}} \end{array}\right.$$【案例代码】LCSMapmap.py12345678910111213141516171819202122232425262728import sysdef cal_lcs(str_1, str_2): len_1 = len(str_1.strip()) len_2 = len(str_2.strip()) tmp = max(len_1, len_2) + 10 len_vv = [[0] * tmp] * tmp for i in range(1, len_1 + 1): for j in range(1, len_2 + 1): if str_1[i - 1] == str_2[j - 1]: len_vv[i][j] = len_vv[i - 1][j - 1] + 1 else: len_vv[i][j] = max(len_vv[i - 1][j], len_vv[i][j - 1]) return float(float(len_vv[len_1][len_2] * 2) / float(len_1 + len_2))for line in sys.stdin: ss = line.strip().split('\t') if len(ss) != 2: continue str_1, str_2 = ss cos_score = cal_lcs(str_1, str_2) print('%s\t%s\t%s' % (str_1, str_2, cos_score))【Hadoop Streaming脚本】1234567891011121314151617181920212223#!/usr/bin/env bashHADOOP_CMD="/usr/local/src/hadoop-2.8.5/bin/hadoop"STREAM_JAR_PATH="/usr/local/src/hadoop-2.8.5/share/hadoop/tools/lib/hadoop-streaming-2.8.5.jar"INPUT_FILE_PATH="/chapter02/02_lcs/lcs_input.data"OUTPUT_PATH="/chapter02/02_lcs/output/python3"LOCAL_FILE_PATH="/mnt/hgfs/Code/chapter02/02_lcs/lcs_input.data"UPLOAD_PATH="/chapter02/02_lcs"$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;INPUT_FILE_PATH&#125;$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;OUTPUT_PATH&#125;$&#123;HADOOP_CMD&#125; fs -mkdir -p $&#123;UPLOAD_PATH&#125;$&#123;HADOOP_CMD&#125; fs -put $&#123;LOCAL_FILE_PATH&#125; $&#123;UPLOAD_PATH&#125;$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -D mapreduce.job.reduces=0 \ -D mapreduce.job.name=lcs_demo \ -files map.py \ -input $&#123;INPUT_FILE_PATH&#125; \ -output $&#123;OUTPUT_PATH&#125; \ -mapper "python map.py" \中文分词中文分词基础背景一段文字不仅仅在于字面上是什么，还在于怎么切分和理解与英文不同，中文词之间没有空格，所以实现中文搜索引擎，比英文多了项分词的任务分词粒度粗粒度：推荐场景细粒度：搜索场景（召回候选）切分方法01bit切开的开始位置对应位是1，否则对应位是0“有/意见/分歧”的bit内容是11010节点序列用分词节点序列表示切分方案“有/意见/分歧”的分词节点序列是{0, 1, 3, 5}基于词典匹配最大长度匹配前向查找后向查找（一般效果较好）数据结构为了提高查找效率，不要逐个匹配词典中的词查找词典所占的时间可能占总的分词时间的1/3左右，为了保证切分速度，需要选择一个好的查找词典方法Trie树常用于加速分词查找词典问题###【Demo】Trie树正向北京大学生活动中心Root北-京北-京-大-学大-学-生学-生生-活活-动中-心结果：北京大学/生活/动/中心反向北京大学生活动中心Root心-中动-活活-生生-学生-学-大学-大-京-北京-北结果：北京/大学生/活动/中心【Demo】DAG词图例：广州本田雅阁汽车【DAG】：{0: [0, 1, 3], 1: [1], 2: [2, 3, 5], 3: [3], 4: [4, 5], 5: [5], 6: [6, 7], 7: [7]}概率语言模型假设需要分出来的词在语料库和词表中都存在，最简单的方法是按词计算概率。从统计思想的角度看，分词问题的输入是一个字串C = c1, c2, …, cN，输出是一个词串S = w1, w2, …, wM，其中M &lt;= N。对于一个特定的字符串C，会有多个切分方案S对应，分词的任务就是从不同S中找出一个切分方案，使得P(S|C)值最大。P(S|C)就是由字符串C产生切分S的概率，也就是对输入字符串切分出最有可能的词序列。$$\operatorname{Seg}(C)=\arg \max _{S \in \mathrm{G}} P(S | C)=\arg \max _{S \in G} \frac{P(C | S) P(S)}{P(C)}$$基础朴素贝叶斯：独立性假设引例C：南京市长江大桥切分方案如下：S1：南京市/长江/大桥S2：南京/市长/江大桥计算条件概率P(S1|C)和P(S2|C)，然后根据P(S1|C)和P(S2|C)的值来决定选择S1还是S2。P(C)是字串在语料库中出现的概率。为容易实现，假设每个词之间的概率是上下文无关的根据朴素贝叶斯公式$$P(C \cap S)=P(S | C)^{\star} P(C)=P(C | S)^{\star} P(S)$$所以条件概率为$$\mathrm{P}(S | C)=\frac{P(C | S) \times P(S)}{P(C)}$$P(C)为常数，只是一个用来归一化的固定值在中文分词中，从词串恢复到汉字串仅有唯一的一种方式，所以P(C|S) = 1综上所述，比较P(S1|C)和P(S2|C)的大小就可以变成比较P(S1)和P(S2)的大小P(S1) = P(南京市, 长江, 大桥) = P(南京市) * P(长江) * P(大桥)P(S2) = P(南京, 市长, 江大桥) = P(南京) * P(市长) * P(江大桥)P(S1) &gt; P(S2)，选择切分方案S1$$\mathrm{P}(\mathrm{S})=\mathrm{P}\left(\mathrm{w}{1}, \mathrm{w}{2}, \ldots, \mathrm{w}{\mathrm{m}}\right) \approx \mathrm{P}\left(\mathrm{w}{1}\right) \times \mathrm{P}\left(\mathrm{w}{2}\right) \times \ldots \times \mathrm{P}\left(\mathrm{w}{\mathrm{m}}\right) \propto \log \mathrm{P}\left(\mathrm{w}{1}\right)+\log \mathrm{P}\left(\mathrm{w}{2}\right)+\ldots+\log \mathrm{P}\left(\mathrm{w}_{\mathrm{m}}\right)$$P(w)就是这个词出现在语料库中的概率。因为函数y = log(x)单调递增，因为词的概率小于1，所以取log后结果是负数。取log为了防止向下溢出结果由乘法转为加法，计算机处理起来速度更快一元模型对于不同的S，M的值都是不一样的，分出的词越多，概率越小（引例）$$\mathrm{P}(w_{i})=\frac{w_{i}在语料库中的出现次数n}{语料库中的总次数N}$$于是$$\log \mathrm{P}\left(\mathrm{w}{\mathrm{i}}\right)=\log \left(\mathrm{Freq}{\mathrm{w}}\right)-\log \mathrm{N}$$这个计算公式也叫做基于一元模型的计算公式，它综合考虑了切分出的词数和词频N元模型给定一个词猜测下一个词。当给定“NBA”时，下一个词会联想到“篮球”，但不太可能会联想到“足球”前后两词出现的概率是相互独立的的假设在实际中是不成立的。N元模型使用n个单词组成的序列来衡量切分方案的合理性，即概率的链规则。P(S) = P(w1, w2, …, wM) = P(w1) * P(w2|w1) * P(w3|w1, w2) * … * P(wM|w1, w2, …, wM-1)Jieba分词工具源码地址：https://github.com/fxsjy/jieba简介分词模式精确模式：将句子最精确的分开，适合文本分析全模式：句子中所有可以成词的词语都扫描出来，速度快，不能解决歧义搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回实现原理语料库：词 + 词频 + 词性基于Trie树结构前缀词典，使用前向遍历实现词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图DAGPython中实现的trie树是基于dict类型的数据结构而且dict中又嵌套dict 类型，这样嵌套很深，导致内存耗费严重前缀词典储存词语及其前缀，如set([&#39;数&#39;, &#39;数据&#39;, &#39;数据结&#39;, &#39;数据结构&#39;])。在句子中按字正向查找词语，在前缀列表中就继续查找，直到不在前缀列表中或超出句子范围。大约比原词库增加40%词条。采用动态规划DP查找最大概率路径，找出基于词频的最大切分组合对于未登录词，基于二元模型，采用基于汉字成词能力的HMM模型，使用Viterbi算法具体步骤给定待分词的句子, 使用正则(re_han)获取匹配的中文字符(和英文字符)切分成的短语列表；利用get_DAG(sentence)函数获得待切分句子的DAG，首先检测(check_initialized)进程是否已经加载词库，若未初始化词库则调用initialize函数进行初始化，initialize中判断有无已经缓存的前缀词典cache_file文件，若有相应的cache文件则直接使用 marshal.load 方法加载前缀词典，若无则通过gen_pfdict对指定的词库dict.txt进行计算生成前缀词典，到jieba进程的初始化工作完成后就调用get_DAG获得句子的DAG；根据cut_block指定具体的方法(__cut_all,__cut_DAG,__cut_DAG_NO_HMM)对每个短语使用DAG进行分词 ，如cut_block=__cut_DAG时则使用DAG(查字典)和动态规划, 得到最大概率路径, 对DAG中那些没有在字典中查到的字, 组合成一个新的片段短语, 使用HMM模型进行分词, 也就是作者说的识别新词, 即识别字典外的新词；使用python的yield 语法生成一个词语生成器, 逐词语返回12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849"""源码地址：https://github.com/ustcdane/annotated_jieba/blob/master/jieba/__init__.py"""# jieba分词的主函数,返回结果是一个可迭代的 generator def cut(self, sentence, cut_all=False, HMM=True): ''' The main function that segments an entire sentence that contains Chinese characters into seperated words. Parameter: - sentence: The str(unicode) to be segmented. - cut_all: Model type. True for full pattern, False for accurate pattern. - HMM: Whether to use the Hidden Markov Model. ''' sentence = strdecode(sentence) # 解码为unicode # 不同模式下的正则 if cut_all: re_han = re_han_cut_all re_skip = re_skip_cut_all else: re_han = re_han_default re_skip = re_skip_default # 设置不同模式下的cut_block分词方法 if cut_all: cut_block = self.__cut_all elif HMM: cut_block = self.__cut_DAG else: cut_block = self.__cut_DAG_NO_HMM # 先用正则对句子进行切分 blocks = re_han.split(sentence) for blk in blocks: if not blk: continue if re_han.match(blk): # re_han匹配的串 for word in cut_block(blk):# 根据不同模式的方法进行分词 yield word else:# 按照re_skip正则表对blk进行重新切分 tmp = re_skip.split(blk)# 返回list for x in tmp: if re_skip.match(x): yield x elif not cut_all: # 精准模式下逐个字符输出 for xx in x: yield xx else: yield x前缀词典对应代码中Tokenizer.FREQ字典12345678910111213141516171819def gen_pfdict(self, f_name): lfreq = &#123;&#125; # 字典存储 词条:出现次数 ltotal = 0 # 所有词条的总的出现次数 with open(f_name, 'rb') as f: # 打开文件 dict.txt for lineno, line in enumerate(f, 1): # 行号,行 try: line = line.strip().decode('utf-8') # 解码为Unicode word, freq = line.split(' ')[:2] # 获得词条 及其出现次数 freq = int(freq) lfreq[word] = freq ltotal += freq for ch in xrange(len(word)):# 处理word的前缀 wfrag = word[:ch + 1] if wfrag not in lfreq: # word前缀不在lfreq则其出现频次置0 lfreq[wfrag] = 0 except ValueError: raise ValueError( 'invalid dictionary entry in %s at Line %s: %s' % (f_name, lineno, line)) return lfreq, ltotalDAG对一个sentence DAG是以{key:list[i,j…], …}的字典结构存储, key是词的在sentence中的位置, list存放的是sentence中以位置key开始的可能的词语的结束位置.12345678910111213141516171819# DAG中是以&#123;key:list,...&#125;的字典结构存储# key是字的开始位置def get_DAG(self, sentence): self.check_initialized() DAG = &#123;&#125; N = len(sentence) for k in xrange(N): tmplist = [] i = k frag = sentence[k] while i &lt; N and frag in self.FREQ: if self.FREQ[frag]: tmplist.append(i) i += 1 frag = sentence[k:i + 1] if not tmplist: tmplist.append(k) DAG[k] = tmplist return DAG基于词频最大切分组合有了词库(dict.txt)的前缀字典和待分词句子sentence的DAG，使用动态规划方法，从后往前遍历，选择一个频度得分最大的一个切分组合。123456789101112131415#动态规划，计算最大概率的切分组合 def calc(self, sentence, DAG, route): N = len(sentence) route[N] = (0, 0) # 对概率值取对数之后的结果(可以让概率相乘的计算变成对数相加,防止相乘造成下溢) logtotal = log(self.total) # 从后往前遍历句子 反向计算最大概率 for idx in xrange(N - 1, -1, -1): # 列表推倒求最大概率对数路径 # route[idx] = max([ (概率对数，词语末字位置) for x in DAG[idx] ]) # 以idx:(概率对数最大值，词语末字位置)键值对形式保存在route中 # route[x+1][0] 表示 词路径[x+1,N-1]的最大概率对数, # [x+1][0]即表示取句子x+1位置对应元组(概率对数，词语末字位置)的概率对数 route[idx] = max((log(self.FREQ.get(sentence[idx:x + 1]) or 1) - logtotal + route[x + 1][0], x) for x in DAG[idx])例：广州本田雅阁汽车P(车) = -8.80006921905【语料库“车”词频取log】P(汽) = -12.897543648【语料库“汽”词频取log】P(汽车) = -8.78007494718【语料库“汽车”词频取log】P(汽车) &gt; P(汽) + P(车)，所以Route概率使用P(汽车)……123456789Route: &#123;0: (-33.271126717488308, 1), 1: (-32.231489259807965, 1), 2: (-23.899234625632083, 5), 3: (-31.523246813843940, 3), 4: (-22.214895405024865, 5), 5: (-19.008467873683230, 5), 6: (-8.7800749471799175, 7), 7: (-8.8000692190498415, 7), 8: (0.0, '')&#125;隐马尔可夫模型HMM马尔可夫链一个随机过程模型，它表述了一系列可能的事件，在这个系列当中每一个事件的概率仅依赖于前一个事件。参数状态：由数字表示，假设共有M个初始概率：$\pi_{k}=P\left(S_{1}=k\right) \quad \mathrm{k}=1,2, \ldots, \mathrm{M}$状态转移概率：$a_{k, l}=P\left(S_{t+1}=l | S_{t}=k\right) \quad \mathrm{k}, 1=1,2, \ldots, \mathrm{M}$例子：天气状态：{晴天, 雨天, 多云, ……}初始概率：P(晴天), P(雨天), ……状态转移概率：P(晴天|雨天), P(雨天|多云), ……参数估计最大似然法初始概率：P(S1 = k) = (k作为序列开始的次数) / (观测序列总数)状态转移概率：P(St + 1 = l |St = k) = (l紧跟k出现的次数) / (k出现的总次数)小结马尔可夫链是对一个序列数据建模隐马尔可夫模型HMM 是一个关于时序的概率模型，它的变量分为两组：状态变量 (隐变量)：由马尔可夫链随机生成观测变量 ：状态序列每个状态对应生成一个观测结果状态变量和观测变量各自都是一个时间序列，每个状态/观测值都和一个时刻相对应假设1：假设隐藏的马尔可夫链在任意时刻 tt 的状态只依赖于前一个时刻（t − 1时）的状态，与其他时刻的状态及观测无关，也与时刻 t 无关。假设2：假设任意时刻的观测只依赖于该时刻的马尔可夫链状态，与其他观测及状态无关。参数状态：由数字表示，假设共有M个，状态序列S观测：由数字表示，假设共有N个，观测序列O状态转移概率：$由a_{k, l}表示, 通常记作矩阵A$发射概率：$b_{k}(u)=\mathrm{P}\left(O_{t}=u | S_{t}=k\right) \quad u=1,2, \ldots, N ; k=1,2, \dots, M, 通常记作矩阵B$初始概率：$由\pi_{k}表示$HMM解决三类问题概率计算问题（又称评价问题）已知：模型$\lambda=[A, B, \pi]$观测序列O求解：给定观测序列，求它和评估模型之间的匹配度$P(O | \lambda)$方案：前向-后向算法预测问题（又称解码问题）已知：模型$\lambda=[A, B, \pi]$观测序列O求解：给定观测序列，求最有可能与之对应的状态序列S方案：Viterbi算法学习问题（又称训练问题）已知：观测序列O或许会给与之对应的状态序列S求解：训练模型$\lambda=[A, B, \pi]$，使其$P(O | \lambda)$最大，最好地描述观测数据方案：有监督/无监督相关文章：52nlpJieba HMM参数初始概率BEMS位置信息词性例: {(‘B’, ‘mq’): -6.78695300139688, ……}转移概率例: {(‘B’, ‘ad’): {(‘E’, ‘ad’): -0.0007479013978476627}, ……}发射概率例: {(‘B’, ‘df’): {u’不’: 0.0}……}Viterbi算法实现12345678910111213141516171819202122232425#状态转移矩阵，比如B状态前只可能是E或S状态 PrevStatus = &#123; 'B':('E','S'), 'M':('M','B'), 'S':('S','E'), 'E':('B','M') &#125; def viterbi(obs, states, start_p, trans_p, emit_p): V = [&#123;&#125;] # 状态概率矩阵 path = &#123;&#125; for y in states: # 初始化状态概率 V[0][y] = start_p[y] + emit_p[y].get(obs[0], MIN_FLOAT) path[y] = [y] # 记录路径 for t in xrange(1, len(obs)): V.append(&#123;&#125;) newpath = &#123;&#125; for y in states: em_p = emit_p[y].get(obs[t], MIN_FLOAT) # t时刻状态为y的最大概率(从t-1时刻中选择到达时刻t且状态为y的状态y0) (prob, state) = max([(V[t - 1][y0] + trans_p[y0].get(y, MIN_FLOAT) + em_p, y0) for y0 in PrevStatus[y]]) V[t][y] = prob newpath[y] = path[state] + [y] # 只保存概率最大的一种路径 path = newpath # 求出最后一个字哪一种状态的对应概率最大，最后一个字只可能是两种情况：E(结尾)和S(独立词) (prob, state) = max((V[len(obs) - 1][y], y) for y in 'ES')推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on Yarn]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>复习笔记</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>TF-IDF</tag>
        <tag>LCS</tag>
        <tag>HMM</tag>
        <tag>中文分词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BigData复习笔记01：HDFS1.0与MapReduce]]></title>
    <url>%2Fposts%2F34c278f3%2F</url>
    <content type="text"><![CDATA[HDFS1.0与MapReduce![troy-t-9sQgt_cR50c-unsplash](https://image.alessa0.cn/124000.jpg)Chapter01. MapReduce &amp; HDFS1.0海量数据分流处理技术分而治之大数据量早期搜索引擎的网页存储系统，单机存储数千万网页，几十亿的网页需要通过几百台单机服务器存储，url为Key分布式文件系统，按Block(64M-256M)来划分组织文件稳定性容错能力数据一致性大流量覆盖的大流量互联网服务南方流量分到电信机房，北方流量分到联通机房搜索引擎将query作为Key来分流大计算根据输入数据划分计算任务MapReduce 按输入数据来划分传统Hash方法如何将大数据流量均分到N台服务器，做到负载均衡？思路：找到合理的Key，Hash(Key)尽量分布均匀Hash(Key) mod N == 0 分到第 0 台Hash(Key) mod N == 1 分到第 1 台……Hash(Key) mod N == i 分到第 i 台……Hash(Key) mod N == N - 1 分到第N - 1台一般以时间戳为Key随机划分一致性Hash支持动态增长， 更高级的划分方法，解决热点(Hot spot)问题案例：服务器A承压50%服务器B承压30%服务器C承压20%如图，用户按Hash(Key)顺时针访问不同服务器。若服务器B挂掉，则服务器B承压30% * 5/7 -&gt; 交付服务器A服务器B承压30% * 2/7 -&gt; 交付服务器CMapReduce用于处理海量数据的分布式计算框架前提：分布式存储架构角色：MasterSlaveClientMapReduce基本思想分而治之分解 &gt;&gt; 求解 &gt;&gt; 合并案例Demo：分面值数钞票方式1: 单点策略一个人数出所有的钞票，数出各面值各有多少张方式2: 分治策略每个人分得一部分钞票，数出各面值有多少张汇总，每个人负责统计一种面值MapReduce计算流程步骤将数据输入到HDFS上对输入数据进行处理对处理的数据进行切片根据就近原则，对切片数据进行对应节点的Map操作，结果暂存在内存缓冲区当缓冲区数据大小到达阈值时锁住缓冲区对切片结果按partition和key进行排序【默认快速排序，第一关键字为分区号，第二关键字为key】，写入磁盘将磁盘上的切片结果进行归并排序{partition, key, value}将Map结果按partition传输到对应Reduce节点Reduce节点将不同Map节点传输的数据按partition分区信息合并，进行Reduce操作结果处理后输出到HDFS详解File文件存储在HDFS中，每个文件切分成多个一定大小（默认64M）的Block，存储在多个DataNode节点上（默认3备份）TextFile（明文标准输出）hadoop fs -cat /xxx查看SequenceFile（二进制输出）hadoop fs -text /xxx查看InputFormatMR框架基础类之一（Java接口）数据分割（Data Splits）每个Split包含后一个Block的开头部分的数据（解决记录跨Block问题）如记录跨跃存储在两个Block中，这条记录属于前一个Block对应的Split记录读取器（Record Reader）将读取到Split导入Map每读取一条记录，将记录作为参数，调用一次Map函数继续这个过程，读取下一条记录直到Split尾部MapShufflePartition， Sort， Spill， Merge， Combiner， Copy， Memory， Disk……性能优化的重点Partition决定数据由哪个Reducer处理，从而分区（如Hash法）MemoryBuffer内存缓冲区，每个Map的结果和Partition处理的Key Value结果都保存在缓存中缓冲区大小：默认100M溢写阈值：100M * 0.8 = 80M缓冲区中的数据：{partition, key, value}三元组Spill内存缓冲区达到阈值时，溢写Spill线程锁住这80M的缓冲区，开始将数据写到本地磁盘中，然后释放内存每次溢写都生成一个数据文件溢出的数据到磁盘前会对数据进行Key排序Sort，以及合并Combiner发送相同Reduce的Key数量，会拼接到一起，减少Partition的索引数量Sort缓冲区数据按照Key进行排序Combiner数据合并，相同Key的数据，Value值合并，减少输出传输量Combiner函数事实上是Reducer函数，满足Combiner处理不影响{sum, max等}最终Reduce等结果时，可以极大提升性能Reduce多个Reduce任务输入的数据都属于不同的Partition，因此结果数据的Key不重复合并Reduce输出文件即可得到最终的结果配置注意事项文件句柄个数ulimit命令报错“当前打开文件超出最大个数”时使用合适的slot单机map、reduce个数【相互隔离】mapred.tasktracker.map.tasks.maximum配置Map的slot数（默认2）mapreduce.tasktracker.tasks.reduce.maximum配置Reduce的slot数（默认2）内存限制Slot数 = CPU核数 - 1多机集群分离磁盘情况适合单机多磁盘（Raid阵列）mapred.local.dirMap中间结果存储路径dfs.data.dirHDFS数据存储路径配置加载简单配置通过提交作业时-file分发复杂较大配置传入HDFSMap中打开文件读取建立内存结构确定Map任务数依次优先参考如下几个原则每个Map任务使用的内存不超过800M， 尽量在500M以下每个Map任务运行时间控制在大约20分钟，最好1-3分钟每个Map任务处理的最大数据量为一个HDFS块大小，一个Map任务处理的输入不能跨文件Map任务总数不能超过平台可用的任务槽位Map要点Map个数为Split份数压缩文件不可切分【通常压缩文件用于控制Map个数】非压缩文件和Sequence文件可以切分dfs.block.size决定block大小确定Reduce任务数依次优先参考如下几个原则每个Reduce任务使用的内存不超过800M， 尽量在500M以下每个Reduce任务运行时间控制在大约20分钟，最好1-3分钟整个Reduce阶段的输入数据总量每个Reduce任务处理的数据量控制在500M以内Map任务数与Reduce任务数乘积输出数据要求Reduce个数设置mapred.reduce.tasks默认为1Reduce个数太少单次执行慢出错再试成本高Reduce个数太大Shuffle开销大输出大量小文件MapReduce重要进程（HDFS1.0）JobTracker主进程，负责接收客户作业提交，调度任务到作业节点上运行，并提供诸如监控工作节点状态及任务进度等管理功能，1个MapReduce集群有1个JobTracker，一般运行在可靠的硬件上TaskTracker是通过周期性的心跳来通知JobTracker其当前的健康状态，每一次心跳包含了可用的map和reduce任务数目，占用的数目及运行中任务的详细信息。JobTracker利用一个线程池来同时处理心跳和客户请求。等待JobClient提交作业TaskTracker由JobTracker指派任务，实例化用户程序，在本地执行任务并周期性地向JobTracker汇报状态。在每一个工作节点上永远只会有1个TaskTracker。每3s主动向JobTracker发送心跳询问有没有任务，如果有，让其派发任务给它执行MapReduce采用多进程并发优点：方便任务资源控制和调配运行稳定缺点：消耗更多的启动时间【不适合低延时作业】MapReduce作业提交流程客户端Client提交作业请求Master的JobTracker接收请求分配Job ID客户端在HDFS对应Job ID目录上传资源Client向JobTracker正式提交任务JobTracker对任务进行初始化JobTracker将HDFS对应Job ID目录文件分发到各个TaskTracker节点TaskTracker向JobTracker发送心跳TaskTracker向HDFS分发Job资源TaskTracker执行任务MapReduce作业调度默认先进先出（FIFO）队列调度模式优先级：very_high, high, normal, low, very lowStreamingMapReduce和HDFS采用Java实现，默认提供Java编程接口Streaming框架允许任何程序语言实现的程序在Hadoop MapReduce中使用Streaming方便已有程序向Hadoop平台移植优点开发效率高方便移植Hadoop平台，仅需按照一定的格式从标准输入读取数据，向标准输出写数据原有单机程序稍加改动即可在Hadoop平台进行分布式处理容易单机调试cat input | mapper | sort | reducer &gt; output便于平台进行资源控制Streaming框架中通过limit等方式可以灵活地限制应用程序使用的内存等资源缺点Streaming默认仅能处理文本数据，如要对二进制数据进行处理，比较好的方法是将二进制的key和value进行base64的编码转换成文本两次数据拷贝和解析（分割），带来一定开销命令行要点input指定作业的输入文件HDFS路径，支持使用*通配符，支持指定多个文件或目录，可多次使用output指定作业的输出文件HDFS路径，路径必须不存在，并且执行作业用户需要具备创建该目录的权限，只能使用一次mapper用户自己写的Map程序reducer用户自己写的Reduce程序file打包本地文件到提交的Job中map和reduce的执行文件map和reduce要用输入的文件，如配置文件类似的配置还有cacheFile 提交HDFS文件到提交的Job中cacheArchive 提交HDFS压缩文件到提交的Job中jobconf提交作业的一些配置属性常见配置mapred.map.tasksmap task数目mapred.reduce.tasksreduce task数目stream.num.map.output.key.field指定map task输出记录中key所占的域数目num.key.dields.for.partition指定对key分出来的前几部分做partition而不是整个keyHDFS1.0HDFS1.0基础HDFS1.0系统架构MasterNameNode管理着文件系统命名空间维护着文件系统树及树中的所有文件和目录存储元数据NameNode保存元信息的种类文件名目录名及它们之间的层级关系文件目录的所有者及其权限每个文件块的名及文件有哪些块组成元数据保存在内存中NameNode元信息并不包含每个块的位置信息保存文件/Block/DataNode之间的映射关系文件名 -&gt; BlockBlock -&gt; DataNode运行NameNode会占用大量内存和I/O资源，一般NameNode不会存储用户数据或执行MapReduce任务全Hadoop系统仅一个NameNode单点问题方案1：将Hadoop元数据写入到本地文件系统时同步到远程挂载的网络文件系统NFS方案2：运行SecondaryNameNode进程，持久化到磁盘SecondaryNameNode并不是NameNode，不取代NameNode也不是NameNode的备份作用是与NameNode交互，定期通过编辑日志文件合并命名空间镜像当NameNode发生故障，NameNode会通过自己合并的命名空间镜像副本来恢复数据SecondaryNameNode保存的NameNode元信息总是滞后于NameNode的，会导致部分数据丢失WorkerDataNode保存Block -&gt; Path的映射关系负责存储数据块，负责为系统客户端提供数据块的读写服务根据NameNode的指示进行创建/删除和复制等操作心跳机制，定期报告文件块列表信息DataNode间通信，进行块的副本处理数据块HDFS默认数据块大小为64M磁盘块一般为512B块增大可以减少寻址时间，降低寻址时间/文件传输时间数据块过大导致整体任务量过小，降低作业处理速度Hadoop更倾向存储大文件一般来说，一条元信息记录会占用200byte内存空间。假设块大小为64M，备份数量是3，那么一个1G大小的文件将占用16 * 3 = 48 个文件块如现在有1000个1M大小的文件，则会占用 1000 * 3 = 3000 个文件块（多个文件不能放到一个块中）如果文件越小，存储同等大小的文件所需要的元信息就越多元信息持久化在NameNode中存放元信息的文件是fsimage在系统运行期间所有对元信息的操作都保存在内存中并被持久化到另一个文件edits中fsimage文件与edits文件会被SecondaryNameNode进程周期性合并机架感知策略默认3副本第一个副本，放在与客户端相同的节点（如客户端是集群外的一台机器，就随机算节点，但是系统会避免挑选太满或太忙的节点）第二个副本，放在不同机架（随机选择）的节点第三个副本，放在与第二个副本同机架但是不同节点上distancedistance = 0, 相同DataNodedistance = 2, 相同Rack下的不同DataNodedistance = 4, 相同IDC下的不同DataNodedistance = 6, 不同IDC下的DataNode数据完整性校验不希望在存储和处理数据时丢失或损坏任何数据HDFS会对写入的数据计算校验和，并在读取时验证校验和两种校验方法校验和检测损坏数据的常用方法时在第一次写入系统时计算数据的校验和，在通道传输过程中，如果新生成的校验和不完全匹配原始的校验和，那么数据就会被认定为是被损坏的（默认512字节创建1个校验码）数据块检测程序DataBlockScanner在DataNode节点上开启一个后台进程，来定期验证存储在它上的所有块，这个是防止物理介质出现损减情况而造成的数据损坏（损坏数据从其他DataNode拷贝）可靠性措施一个名字节点和多个数据节点数据复制（冗余机制）存放位置（机架感知策略）并不是3副本写完才返回ack，三副本中有1个写成功就返回ack故障检测数据节点心跳包（检测是否宕机）块报告（安全模式下检测）数据完整性检测（校验和比较）名字节点日志文件镜像文件空间回收机制Trash目录（修改core-site.xml）HDFS&amp;MapReduce本地模式Master（小集群）NameNodeJobTrackerSlaveDataNodeTaskTracker案例代码常见实践有：数据统计WordCount数据过滤（清洗）从日志查找某一个条件等数据除去非法数据，保留合法数据数据格式整理同类汇聚多份日志，相同时间点、用户行为日志Join类表格文件存储中，相同主键拼接相关属性历史的主数据与新增，修改数据合并全局排序混合日志，按时间排列好顺序按某个或多个字段有序容错框架测试集群状态：在集群上运行一个错误代码Job，进行观察使用易出错的服务，365 * 24 运行计算规模经常变化调整的服务单进程程序，迅速提升执行计算效率WordCountPython3map.py1234567import sysfor line in sys.stdin: ss = line.strip().split(' ') for word in ss: print(word.strip() + '\t' + '1')reduce.py12345678910111213141516171819202122import syscurrent_word = Nonecnt_sum = 0for line in sys.stdin: ss = line.strip().split('\t') if len(ss) != 2: continue word, cnt = ss if current_word is None: current_word = word if current_word != word: print(current_word.strip() + '\t' + str(cnt_sum)) current_word = word cnt_sum = 0 cnt_sum += int(cnt)print(current_word.strip() + '\t' + str(cnt_sum))The_Man_of_Property.txt1“The Forsyte Saga” was the title originally destined for that part of it which is called “The Man of Property”; and to adopt it for the collected chronicles of the Forsyte family has indulged the Forsytean tenacity that is in all of us. The word Saga might be objected to on the ground that it connotes the heroic and that there is little heroism in these pages. But it is used with a suitable irony; and, after all, this long tale, though it may deal with folk in frock coats, furbelows, and a gilt-edged period, is not devoid of the essential heat of conflict. Discounting for the gigantic stature and blood-thirstiness of old days, as they have come down to us in fairy-tale and legend, the folk of the old Sagas were Forsytes, assuredly, in their possessive instincts, and as little proof against the inroads of beauty and passion as Swithin, Soames, or even Young Jolyon. And if heroic figures, in days that never were, seem to startle out from their surroundings in fashion unbecoming to a Forsyte of the Victorian era, we may be sure that tribal instinct was even then the prime force, and that “family” and the sense of home and property counted as they do to this day, for all the recent efforts to “talk them out.”run.sh12345678910111213141516171819202122232425262728293031323334353637#!/usr/bin/env bash# hadoop命令地址HADOOP_CMD="/usr/local/src/hadoop-2.8.5/bin/hadoop"# hadoop streaming jar包地址STREAM_JAR_PATH="/usr/local/src/hadoop-2.8.5/share/hadoop/tools/lib/hadoop-streaming-2.8.5.jar"# HDFS 输入文件路径INPUT_FILE_PATH="/week01/01_mr_wordcount/The_Man_of_Property.txt"# HDFS 输出文件路径OUTPUT_PATH="/week01/01_mr_wordcount/output/python3"# 输入文件本地路径LOCAL_FILE_PATH="/mnt/hgfs/Code/week01/01_mr_wordcount/The_Man_of_Property.txt"# 输入文件 HDFS上传路径UPLOAD_PATH="/week01/01_mr_wordcount"# 删除HDFS存在目录$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;INPUT_FILE_PATH&#125;$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;OUTPUT_PATH&#125;# 创建HDFS 上传目录$&#123;HADOOP_CMD&#125; fs -mkdir -p $&#123;UPLOAD_PATH&#125;# 将本地输入文件上传到HDFS目录$&#123;HADOOP_CMD&#125; fs -put $&#123;LOCAL_FILE_PATH&#125; $&#123;UPLOAD_PATH&#125;# 命令行$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -files map.py,red.py \ -input $&#123;INPUT_FILE_PATH&#125; \ -output $&#123;OUTPUT_PATH&#125; \ -mapper "python map.py" \ -reducer "python red.py" \ # -file 过时了，2.8.5用-files代替，作为可选参数需放在-input等参数前面AllSort_1ReduceVersion 1Python3map_sort.py1234567891011import sysbase_count = 10000for line in sys.stdin: ss = line.strip().split('\t') key, val = ss new_key = base_count + int(key) print(str(new_key) + '\t' + val)red_sort.py12345678910import sysbase_count = 10000for line in sys.stdin: new_key, val = line.strip().split('\t') key = int(new_key) - base_count print('%s\t%s' % (key, val))a.txt123451 hadoop3 hadoop5 hadoop7 hadoop9 hadoopb.txt123450 java2 java4 java6 java8 javarun.sh12345678910111213141516171819202122232425262728293031#!/usr/bin/env bashHADOOP_CMD="/usr/local/src/hadoop-2.8.5/bin/hadoop"STREAM_JAR_PATH="/usr/local/src/hadoop-2.8.5/share/hadoop/tools/lib/hadoop-streaming-2.8.5.jar"INPUT_FILE_PATH_A="/week01/02_mr_allsort_1reduce/a.txt"INPUT_FILE_PATH_B="/week01/02_mr_allsort_1reduce/b.txt"OUTPUT_PATH="/week01/02_mr_allsort_1reduce/version1/output/python3"LOCAL_FILE_PATH_A="/mnt/hgfs/Code/week01/02_mr_allsort_1reduce/a.txt"LOCAL_FILE_PATH_B="/mnt/hgfs/Code/week01/02_mr_allsort_1reduce/b.txt"UPLOAD_PATH="/week01/02_mr_allsort_1reduce/"$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;INPUT_FILE_PATH_A&#125;$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;INPUT_FILE_PATH_B&#125;$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;OUTPUT_PATH&#125;$&#123;HADOOP_CMD&#125; fs -mkdir -p $&#123;UPLOAD_PATH&#125;$&#123;HADOOP_CMD&#125; fs -put $&#123;LOCAL_FILE_PATH_A&#125; $&#123;LOCAL_FILE_PATH_B&#125; $&#123;UPLOAD_PATH&#125;$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -D mapreduce.job.reduces=1 \ -files map_sort.py,red_sort.py \ -input $&#123;INPUT_FILE_PATH_A&#125;,$&#123;INPUT_FILE_PATH_B&#125; \ -output $&#123;OUTPUT_PATH&#125; \ -mapper "python map_sort.py" \ -reducer "python red_sort.py" \ # 依赖MapReduce框架自身的sort功能： # -D mapreduce.job.reduces=1 # -jobconf 可用-D 替代，作为可选参数放在-input等前面Version 2Python3map_sort.py12345import sysfor line in sys.stdin: print(line.strip())red_sort.py12345import sysfor line in sys.stdin: print(line.strip())a.txt123451 hadoop3 hadoop5 hadoop7 hadoop9 hadoopb.txt123450 java2 java4 java6 java8 javarun.sh123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/usr/bin/env bashHADOOP_CMD="/usr/local/src/hadoop-2.8.5/bin/hadoop"STREAM_JAR_PATH="/usr/local/src/hadoop-2.8.5/share/hadoop/tools/lib/hadoop-streaming-2.8.5.jar"INPUT_FILE_PATH_A="/week01/02_mr_allsort_1reduce/a.txt"INPUT_FILE_PATH_B="/week01/02_mr_allsort_1reduce/b.txt"OUTPUT_PATH="/week01/02_mr_allsort_1reduce/version2/output/python3"LOCAL_FILE_PATH_A="/mnt/hgfs/Code/week01/02_mr_allsort_1reduce/a.txt"LOCAL_FILE_PATH_B="/mnt/hgfs/Code/week01/02_mr_allsort_1reduce/b.txt"UPLOAD_PATH="/week01/02_mr_allsort_1reduce/"$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;INPUT_FILE_PATH_A&#125;$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;INPUT_FILE_PATH_B&#125;$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;OUTPUT_PATH&#125;$&#123;HADOOP_CMD&#125; fs -mkdir -p $&#123;UPLOAD_PATH&#125;$&#123;HADOOP_CMD&#125; fs -put $&#123;LOCAL_FILE_PATH_A&#125; $&#123;LOCAL_FILE_PATH_B&#125; $&#123;UPLOAD_PATH&#125;$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \ -D stream.num.map.output.key.fields=1 \ -D mapreduce.partition.keypartitioner.options="-k1,1" \ -D mapreduce.partition.keycomparator.options="-k1,1n" \ -D mapreduce.job.reduces=1 \ -files map_sort.py,red_sort.py \ -input $&#123;INPUT_FILE_PATH_A&#125;,$&#123;INPUT_FILE_PATH_B&#125; \ -output $&#123;OUTPUT_PATH&#125; \ -mapper "python map_sort.py" \ -reducer "python red_sort.py" \ -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \ # 设置单reduce： # -D mapreduce.job.reduces=1 \ # 控制分发，完成二次排序： # -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \ # 完成key排序： # -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \ # 设置分隔符位置（默认\t）,分隔符之前为key，之后为value # -D stream.num.map.output.key.fields=1 \ # 选择哪一部分做partition，-k1,1表示partition的key范围是（1，1），即第1列 # -D mapreduce.partition.keypartitioner.options="-k1,1" \ # 设置key中需要比较的字段或字节范围，-k1,1表示sort的key范围是（1，1），即第1列，n表示按数字number类型排序 # -D mapreduce.partition.keycomparator.options="-k1,1n" \AllSortPython3map_sort.py12345678910111213141516import sysbase_count = 10000for line in sys.stdin: ss = line.strip().split('\t') key, val = ss new_key = base_count + int(key) partition_index = 1 if new_key &lt; (10100 + 10000) / 2: partition_index = 0 print("%s\t%s\t%s" % (partition_index, new_key, val))red_sort.py12345678910import sysbase_count = 10000for line in sys.stdin: partition_index, new_key, val = line.strip().split('\t') key = int(new_key) - base_count print('\t'.join([str(key), val]))a.txt123451 hadoop3 hadoop5 hadoop7 hadoop9 hadoopb.txt123450 java2 java4 java6 java8 javarun.sh12345678910111213141516171819202122232425262728293031323334353637#!/usr/bin/env bashHADOOP_CMD="/usr/local/src/hadoop-2.8.5/bin/hadoop"STREAM_JAR_PATH="/usr/local/src/hadoop-2.8.5/share/hadoop/tools/lib/hadoop-streaming-2.8.5.jar"INPUT_FILE_PATH_A="/week01/03_mr_allsort/a.txt"INPUT_FILE_PATH_B="/week01/03_mr_allsort/b.txt"OUTPUT_PATH="/week01/03_mr_allsort/output/python3"LOCAL_FILE_PATH_A="/mnt/hgfs/Code/week01/03_mr_allsort/a.txt"LOCAL_FILE_PATH_B="/mnt/hgfs/Code/week01/03_mr_allsort/b.txt"UPLOAD_PATH="/week01/03_mr_allsort/"$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;INPUT_FILE_PATH_A&#125;$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;INPUT_FILE_PATH_B&#125;$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;OUTPUT_PATH&#125;$&#123;HADOOP_CMD&#125; fs -mkdir -p $&#123;UPLOAD_PATH&#125;$&#123;HADOOP_CMD&#125; fs -put $&#123;LOCAL_FILE_PATH_A&#125; $&#123;LOCAL_FILE_PATH_B&#125; $&#123;UPLOAD_PATH&#125;$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -D mapreduce.job.reduces=2 \ -D stream.num.map.output.key.fields=2 \ -D num.key.fields.for.partition=1 \ -files map_sort.py,red_sort.py \ -input $&#123;INPUT_FILE_PATH_A&#125;,$&#123;INPUT_FILE_PATH_B&#125; \ -output $&#123;OUTPUT_PATH&#125; \ -mapper "python map_sort.py" \ -reducer "python red_sort.py" \ -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner # 设置分隔符位置（默认\t）,分隔符之前为key，之后为value，此处以前2列为key # -D stream.num.map.output.key.fields=2 \ # 设置partition key（仅能从头顺序选取范围）用来做分发，此处以第1列做partition # -D num.key.fields.for.partition=1 \ # 设置partition key（可选择中间范围）用来做分发 # -D mapreduce.partition.keypartitioner.options="-k1,1" \File_BroadcastFilePython3map.py123456789101112131415161718192021222324252627282930import sysdef read_local_file_func(f): word_set = set() file_in = open(f, 'r') for line in file_in: word = line.strip() word_set.add(word) return word_setdef mapper_func(white_list_fd): word_set = read_local_file_func(white_list_fd) for line in sys.stdin: ss = line.strip().split(' ') for s in ss: word = s.strip() if word != "" and (word in word_set): print("%s\t%s" % (s, 1))if __name__ == "__main__": module = sys.modules[__name__] func = getattr(module, sys.argv[1]) args = None if len(sys.argv) &gt; 1: args = sys.argv[2:] func(*args)red.py123456789101112131415161718192021222324252627282930313233343536import sysdef reducer_func(): current_word = None count_pool = [] sum = 0 for line in sys.stdin: word, val = line.strip().split('\t') if current_word is None: current_word = word if current_word != word: for count in count_pool: sum += count print("%s\t%s" % (current_word, sum)) current_word = word count_pool = [] sum = 0 count_pool.append(int(val)) for count in count_pool: sum += count print("%s\t%s" % (current_word, str(sum)))if __name__ == "__main__": module = sys.modules[__name__] func = getattr(module, sys.argv[1]) args = None if len(sys.argv) &gt; 1: args = sys.argv[2:] func(*args)The_Man_of_Property.txt1“The Forsyte Saga” was the title originally destined for that part of it which is called “The Man of Property”; and to adopt it for the collected chronicles of the Forsyte family has indulged the Forsytean tenacity that is in all of us. The word Saga might be objected to on the ground that it connotes the heroic and that there is little heroism in these pages. But it is used with a suitable irony; and, after all, this long tale, though it may deal with folk in frock coats, furbelows, and a gilt-edged period, is not devoid of the essential heat of conflict. Discounting for the gigantic stature and blood-thirstiness of old days, as they have come down to us in fairy-tale and legend, the folk of the old Sagas were Forsytes, assuredly, in their possessive instincts, and as little proof against the inroads of beauty and passion as Swithin, Soames, or even Young Jolyon. And if heroic figures, in days that never were, seem to startle out from their surroundings in fashion unbecoming to a Forsyte of the Victorian era, we may be sure that tribal instinct was even then the prime force, and that “family” and the sense of home and property counted as they do to this day, for all the recent efforts to “talk them out.”white_list1therun.sh1234567891011121314151617181920212223#!/usr/bin/env bashHADOOP_CMD="/usr/local/src/hadoop-2.8.5/bin/hadoop"STREAM_JAR_PATH="/usr/local/src/hadoop-2.8.5/share/hadoop/tools/lib/hadoop-streaming-2.8.5.jar"INPUT_FILE_PATH="/week01/04_mr_file_broadcast/The_Man_of_Property.txt"OUTPUT_PATH="/week01/04_mr_file_broadcast/file/output/python3"LOCAL_FILE_PATH="/mnt/hgfs/Code/week01/04_mr_file_broadcast/The_Man_of_Property.txt"UPLOAD_PATH="/week01/04_mr_file_broadcast/"$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;INPUT_FILE_PATH&#125;$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;OUTPUT_PATH&#125;$&#123;HADOOP_CMD&#125; fs -mkdir -p $&#123;UPLOAD_PATH&#125;$&#123;HADOOP_CMD&#125; fs -put $&#123;LOCAL_FILE_PATH&#125; $&#123;UPLOAD_PATH&#125;$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -D mapreduce.job.reduces=2 \ -files map.py,red.py,white_list \ -input $&#123;INPUT_FILE_PATH&#125; \ -output $&#123;OUTPUT_PATH&#125; \ -mapper "python map.py mapper_func white_list" \ -reducer "python red.py reducer_func" \cacheFilePython3map.py123456789101112131415161718192021222324252627282930import sysdef read_local_file_func(file): word_set = set() file_in = open(file, 'r') for line in file_in: word = line.strip() word_set.add(word) return word_setdef mapper_func(white_list): word_set = read_local_file_func(white_list) for line in sys.stdin: ss = line.strip().split(' ') for s in ss: word = s.strip() if word != "" and (word in word_set): print("%s\t%s" % (s, 1))if __name__ == "__main__": module = sys.modules[__name__] func = getattr(module, sys.argv[1]) args = None if len(sys.argv) &gt; 1: args = sys.argv[2:] func(*args)red.py1234567891011121314151617181920212223242526272829303132import sysdef reducer_func(): current_word = None cnt_sum = 0 for line in sys.stdin: ss = line.strip().split('\t') if len(ss) != 2: continue word, cnt = ss if current_word is None: current_word = word if current_word != word: print(current_word.strip() + '\t' + str(cnt_sum)) current_word = word cnt_sum = 0 cnt_sum += int(cnt) print(current_word.strip() + '\t' + str(cnt_sum))if __name__ == "__main__": module = sys.modules[__name__] func = getattr(module, sys.argv[1]) args = None if len(sys.argv) &gt; 1: args = sys.argv[2:] func(*args)The_Man_of_Property.txt1“The Forsyte Saga” was the title originally destined for that part of it which is called “The Man of Property”; and to adopt it for the collected chronicles of the Forsyte family has indulged the Forsytean tenacity that is in all of us. The word Saga might be objected to on the ground that it connotes the heroic and that there is little heroism in these pages. But it is used with a suitable irony; and, after all, this long tale, though it may deal with folk in frock coats, furbelows, and a gilt-edged period, is not devoid of the essential heat of conflict. Discounting for the gigantic stature and blood-thirstiness of old days, as they have come down to us in fairy-tale and legend, the folk of the old Sagas were Forsytes, assuredly, in their possessive instincts, and as little proof against the inroads of beauty and passion as Swithin, Soames, or even Young Jolyon. And if heroic figures, in days that never were, seem to startle out from their surroundings in fashion unbecoming to a Forsyte of the Victorian era, we may be sure that tribal instinct was even then the prime force, and that “family” and the sense of home and property counted as they do to this day, for all the recent efforts to “talk them out.”white_list1therun.sh123456789101112131415161718192021222324252627282930313233#!/usr/bin/env bashHADOOP_CMD="/usr/local/src/hadoop-2.8.5/bin/hadoop"STREAM_JAR_PATH="/usr/local/src/hadoop-2.8.5/share/hadoop/tools/lib/hadoop-streaming-2.8.5.jar"INPUT_FILE_PATH="/week01/04_mr_file_broadcast/The_Man_of_Property.txt"OUTPUT_PATH="/week01/04_mr_file_broadcast/cachefile/output/python3"LOCAL_FILE_PATH_A="/mnt/hgfs/Code/week01/04_mr_file_broadcast/The_Man_of_Property.txt"LOCAL_FILE_PATH_B="/mnt/hgfs/Code/week01/04_mr_file_broadcast/white_list"UPLOAD_PATH_A="/week01/04_mr_file_broadcast/"UPLOAD_PATH_B="/week01/04_mr_file_broadcast/cachefile/"$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;INPUT_FILE_PATH&#125;$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;OUTPUT_PATH&#125;$&#123;HADOOP_CMD&#125; fs -mkdir -p $&#123;UPLOAD_PATH_B&#125;$&#123;HADOOP_CMD&#125; fs -put $&#123;LOCAL_FILE_PATH_A&#125; $&#123;UPLOAD_PATH_A&#125;$&#123;HADOOP_CMD&#125; fs -put $&#123;LOCAL_FILE_PATH_B&#125; $&#123;UPLOAD_PATH_B&#125;$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -D mapreduce.job.reduces=2 \ -D mapreduce.job.name=cachefile_demo \ -files map.py,red.py,"hdfs://master:9000/week01/04_mr_file_broadcast/cachefile/white_list#WH" \ -input $&#123;INPUT_FILE_PATH&#125; \ -output $&#123;OUTPUT_PATH&#125; \ -mapper "python map.py mapper_func WH" \ -reducer "python red.py reducer_func" \ # 过时了 # -cacheFile "hdfs://master:9000/week01/04_mr_file_broadcast/cachefile # /white_list#WWWHHH" \ #-cacheFile "$&#123;HDFS_FILE_PATH&#125;#WH" \cacheArchivePython3map.py123456789101112131415161718192021222324252627282930313233343536373839404142434445import osimport sysimport gzipdef get_file_handler(f): file_in = open(f, 'r') return file_indef get_cachefile_handlers(f): f_handlers_list = [] if os.path.isdir(f): for fd in os.listdir(f): f_handlers_list.append(get_file_handler(f + '/' + fd)) return f_handlers_listdef read_local_file_func(f): word_set = set() for cachefile in get_cachefile_handlers(f): for line in cachefile: word = line.strip() word_set.add(word) return word_setdef mapper_func(white_list_fd): word_set = read_local_file_func(white_list_fd) for line in sys.stdin: ss = line.strip().split(' ') for s in ss: word = s.strip() if word != "" and (word in word_set): print("%s\t%s" % (s, 1))if __name__ == "__main__": module = sys.modules[__name__] func = getattr(module, sys.argv[1]) args = None if len(sys.argv) &gt; 1: args = sys.argv[2:] func(*args)red.py123456789101112131415161718192021222324252627282930313233343536import sysdef reducer_func(): current_word = None count_pool = [] sum = 0 for line in sys.stdin: word, val = line.strip().split('\t') if current_word is None: current_word = word if current_word != word: for count in count_pool: sum += count print("%s\t%s" % (current_word, sum)) current_word = word count_pool = [] sum = 0 count_pool.append(int(val)) for count in count_pool: sum += count print("%s\t%s" % (current_word, str(sum)))if __name__ == "__main__": module = sys.modules[__name__] func = getattr(module, sys.argv[1]) args = None if len(sys.argv) &gt; 1: args = sys.argv[2:] func(*args)The_Man_of_Property.txt1“The Forsyte Saga” was the title originally destined for that part of it which is called “The Man of Property”; and to adopt it for the collected chronicles of the Forsyte family has indulged the Forsytean tenacity that is in all of us. The word Saga might be objected to on the ground that it connotes the heroic and that there is little heroism in these pages. But it is used with a suitable irony; and, after all, this long tale, though it may deal with folk in frock coats, furbelows, and a gilt-edged period, is not devoid of the essential heat of conflict. Discounting for the gigantic stature and blood-thirstiness of old days, as they have come down to us in fairy-tale and legend, the folk of the old Sagas were Forsytes, assuredly, in their possessive instincts, and as little proof against the inroads of beauty and passion as Swithin, Soames, or even Young Jolyon. And if heroic figures, in days that never were, seem to startle out from their surroundings in fashion unbecoming to a Forsyte of the Victorian era, we may be sure that tribal instinct was even then the prime force, and that “family” and the sense of home and property counted as they do to this day, for all the recent efforts to “talk them out.”w.tar.gzwhite_list_11thewhite_list_21arun.sh12345678910111213141516171819202122232425262728#!/usr/bin/env bashHADOOP_CMD="/usr/local/src/hadoop-2.8.5/bin/hadoop"STREAM_JAR_PATH="/usr/local/src/hadoop-2.8.5/share/hadoop/tools/lib/hadoop-streaming-2.8.5.jar"INPUT_FILE_PATH="/week01/04_mr_file_broadcast/The_Man_of_Property.txt"OUTPUT_PATH="/week01/04_mr_file_broadcast/cachearchive/output/python3"LOCAL_FILE_PATH_A="/mnt/hgfs/Code/week01/04_mr_file_broadcast/The_Man_of_Property.txt"LOCAL_FILE_PATH_B="/mnt/hgfs/Code/week01/04_mr_file_broadcast/w.tar.gz"UPLOAD_PATH_A="/week01/04_mr_file_broadcast/"UPLOAD_PATH_B="/week01/04_mr_file_broadcast/cachearchive/"$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;INPUT_FILE_PATH&#125;$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;OUTPUT_PATH&#125;$&#123;HADOOP_CMD&#125; fs -mkdir -p $&#123;UPLOAD_PATH_B&#125;$&#123;HADOOP_CMD&#125; fs -put $&#123;LOCAL_FILE_PATH_A&#125; $&#123;UPLOAD_PATH_A&#125;$&#123;HADOOP_CMD&#125; fs -put $&#123;LOCAL_FILE_PATH_B&#125; $&#123;UPLOAD_PATH_B&#125;$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -D mapreduce.job.reduces=2 \ -D mapreduce.job.name=cachearchive_demo \ -files map.py,red.py \ -archives "hdfs://master:9000/week01/04_mr_file_broadcast/cachearchive/w.tar.gz#WH.gz" \ -input $&#123;INPUT_FILE_PATH&#125; \ -output $&#123;OUTPUT_PATH&#125; \ -mapper "python map.py mapper_func WH.gz" \ -reducer "python red.py reducer_func" \CompressionPython3map.py123456789101112131415161718192021222324252627282930313233343536373839404142434445import osimport sysimport gzipdef get_file_handler(f): file_in = open(f, 'r') return file_indef get_cachefile_handlers(f): f_handlers_list = [] if os.path.isdir(f): for fd in os.listdir(f): f_handlers_list.append(get_file_handler(f + '/' + fd)) return f_handlers_listdef read_local_file_func(f): word_set = set() for cachefile in get_cachefile_handlers(f): for line in cachefile: word = line.strip() word_set.add(word) return word_setdef mapper_func(white_list_fd): word_set = read_local_file_func(white_list_fd) for line in sys.stdin: ss = line.strip().split(' ') for s in ss: word = s.strip() if word != "" and (word in word_set): print("%s\t%s" % (s, 1))if __name__ == "__main__": module = sys.modules[__name__] func = getattr(module, sys.argv[1]) args = None if len(sys.argv) &gt; 1: args = sys.argv[2:] func(*args)red.py123456789101112131415161718192021222324252627282930313233343536import sysdef reducer_func(): current_word = None count_pool = [] sum = 0 for line in sys.stdin: word, val = line.strip().split('\t') if current_word is None: current_word = word if current_word != word: for count in count_pool: sum += count print("%s\t%s" % (current_word, sum)) current_word = word count_pool = [] sum = 0 count_pool.append(int(val)) for count in count_pool: sum += count print("%s\t%s" % (current_word, str(sum)))if __name__ == "__main__": module = sys.modules[__name__] func = getattr(module, sys.argv[1]) args = None if len(sys.argv) &gt; 1: args = sys.argv[2:] func(*args)The_Man_of_Property.txt1“The Forsyte Saga” was the title originally destined for that part of it which is called “The Man of Property”; and to adopt it for the collected chronicles of the Forsyte family has indulged the Forsytean tenacity that is in all of us. The word Saga might be objected to on the ground that it connotes the heroic and that there is little heroism in these pages. But it is used with a suitable irony; and, after all, this long tale, though it may deal with folk in frock coats, furbelows, and a gilt-edged period, is not devoid of the essential heat of conflict. Discounting for the gigantic stature and blood-thirstiness of old days, as they have come down to us in fairy-tale and legend, the folk of the old Sagas were Forsytes, assuredly, in their possessive instincts, and as little proof against the inroads of beauty and passion as Swithin, Soames, or even Young Jolyon. And if heroic figures, in days that never were, seem to startle out from their surroundings in fashion unbecoming to a Forsyte of the Victorian era, we may be sure that tribal instinct was even then the prime force, and that “family” and the sense of home and property counted as they do to this day, for all the recent efforts to “talk them out.”white_list_dir.tar.gzwhite_list_11thewhite_list_21arun.sh12345678910111213141516171819202122232425262728293031#!/usr/bin/env bashHADOOP_CMD="/usr/local/src/hadoop-2.8.5/bin/hadoop"STREAM_JAR_PATH="/usr/local/src/hadoop-2.8.5/share/hadoop/tools/lib/hadoop-streaming-2.8.5.jar"INPUT_FILE_PATH="/week01/05_mr_compression/The_Man_of_Property.txt"OUTPUT_PATH="/week01/05_mr_compression/run_1/output/python3"LOCAL_FILE_PATH_A="/mnt/hgfs/Code/week01/05_mr_compression/The_Man_of_Property.txt"LOCAL_FILE_PATH_B="/mnt/hgfs/Code/week01/05_mr_compression/white_list_dir.tar.gz"UPLOAD_PATH="/week01/05_mr_compression/"$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;INPUT_FILE_PATH&#125;$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;OUTPUT_PATH&#125;$&#123;HADOOP_CMD&#125; fs -mkdir -p $&#123;UPLOAD_PATH&#125;$&#123;HADOOP_CMD&#125; fs -put $&#123;LOCAL_FILE_PATH_A&#125; $&#123;LOCAL_FILE_PATH_B&#125; $&#123;UPLOAD_PATH&#125;# Step 1.$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -D mapreduce.job.reduces=2 \ -D mapreduce.job.name=compression_run_1_demo \ -D mapreduce.map.output.compress=true \ -D mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.GzipCodec \ -D mapreduce.output.fileoutputformat.compress=true \ -D mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec \ -files map.py,red.py \ -archives "hdfs://master:9000/week01/05_mr_compression/white_list_dir.tar.gz#WH.gz" \ -input $&#123;INPUT_FILE_PATH&#125; \ -output $&#123;OUTPUT_PATH&#125; \ -mapper "python map.py mapper_func WH.gz" \ -reducer "python red.py reducer_func" \JoinPython3map_a.py1234567import sysfor line in sys.stdin: key, val = line.strip().split(' ') print("%s\t1\t%s" % (key, val))a.txt12345aaa1 123aaa2 123aaa3 123aaa4 123aaa5 123map_b.py1234567import sysfor line in sys.stdin: key, val = line.strip().split(' ') print("%s\t2\t%s" % (key, val))b.txt12345aaa1 hadoopaaa2 hadoopaaa3 hadoopaaa4 hadoopaaa5 hadoopred_join.py1234567891011121314import sysval_1 = ""for line in sys.stdin: key, flag, val = line.strip().split('\t') if flag == '1': val_1 = val elif flag == '2': val_2 = val print("%s\t%s\t%s" % (key, val_1, val_2)) val_1 = ""run.sh123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/usr/bin/env bashHADOOP_CMD="/usr/local/src/hadoop-2.8.5/bin/hadoop"STREAM_JAR_PATH="/usr/local/src/hadoop-2.8.5/share/hadoop/tools/lib/hadoop-streaming-2.8.5.jar"INPUT_FILE_PATH_A="/week01/06_mr_join/a.txt"INPUT_FILE_PATH_B="/week01/06_mr_join/b.txt"OUTPUT_PATH_A="/week01/06_mr_join/output/a/python3"OUTPUT_PATH_B="/week01/06_mr_join/output/b/python3"OUTPUT_PATH_JOIN="/week01/06_mr_join/output/join/python3"LOCAL_FILE_PATH_A="/mnt/hgfs/Code/week01/06_mr_join/a.txt"LOCAL_FILE_PATH_B="/mnt/hgfs/Code/week01/06_mr_join/b.txt"UPLOAD_PATH="/week01/06_mr_join/"$&#123;HADOOP_CMD&#125; fs -rm -r -skipTrash $&#123;INPUT_FILE_PATH_A&#125; $&#123;INPUT_FILE_PATH_B&#125; $&#123;OUTPUT_PATH_A&#125; $&#123;OUTPUT_PATH_B&#125; $&#123;OUTPUT_PATH_JOIN&#125;$&#123;HADOOP_CMD&#125; fs -mkdir -p $&#123;UPLOAD_PATH&#125;$&#123;HADOOP_CMD&#125; fs -put $&#123;LOCAL_FILE_PATH_A&#125; $&#123;LOCAL_FILE_PATH_B&#125; $&#123;UPLOAD_PATH&#125;# Step 1.$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -files map_a.py \ -input $&#123;INPUT_FILE_PATH_A&#125; \ -output $&#123;OUTPUT_PATH_A&#125; \ -mapper "python map_a.py" \# Step 2.$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -files map_b.py \ -input $&#123;INPUT_FILE_PATH_B&#125; \ -output $&#123;OUTPUT_PATH_B&#125; \ -mapper "python map_b.py" \# Step 3.$&#123;HADOOP_CMD&#125; jar $&#123;STREAM_JAR_PATH&#125; \ -D stream.num.map.output.key.fields=2 \ -D num.key.fields.for.partition=1 \ -files red_join.py \ -input $&#123;OUTPUT_PATH_A&#125;,$&#123;OUTPUT_PATH_B&#125; \ -output $&#123;OUTPUT_PATH_JOIN&#125; \ -mapper "cat" \ -reducer "python red_join.py" \附加项目：pywebPython3首先请确保使用pip install web.py==0.40-dev1main.py123456789101112131415161718192021222324252627282930313233343536373839import webimport sysurls = ( '/', 'index', '/test', 'test',)app = web.application(urls, globals())userid_rec_dict = &#123;&#125;with open('file.test', 'r') as fd: for line in fd: ss = line.strip().split('\t') if len(ss) != 2: continue userid = ss[0].strip() items = ss[1].strip() userid_rec_dict[userid] = itemsclass index: def GET(self): params = web.input() userid = params.get('userid', '') if userid not in userid_rec_dict: return 'no rec!' else: return '\n'.join(userid_rec_dict[userid].strip().split(''))class test: def GET(self): print(web.input()) return '222'if __name__ == "__main__": app.run()file.test123zhangsan 1lisi 2wangwu 3在终端输入python main.py 12345启动web服务器如遇到报错信息123456789101112131415Traceback (most recent call last): File "D:\Program Files\Python\Python37\lib\site-packages\web\utils.py", line 526, in take yield next(seq)StopIterationThe above exception was the direct cause of the following exception:Traceback (most recent call last): File "D:\Python\hello.py", line 6, in &lt;module&gt; app = web.application(urls, globals(),True) File "D:\Program Files\Python\Python37\lib\site-packages\web\application.py", line 62, in __init__ self.init_mapping(mapping) File "D:\Program Files\Python\Python37\lib\site-packages\web\application.py", line 130, in init_mapping self.mapping = list(utils.group(mapping, 2)) File "D:\Program Files\Python\Python37\lib\site-packages\web\utils.py", line 531, in group x = list(take(seq, size))RuntimeError: generator raised StopIteration修改Lib\site-packages\web 下的utils.py文件1234+ try: yield next(seq) # 526行+ except StopIteration:+ return在网页打开http://0.0.0.0:12345/即可访问页面输入网址http://0.0.0.0:12345/?userid=zhangsan页面显示1未来可拓展推荐系统 远程分词服务等推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on Yarn]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>复习笔记</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>HDFS1.0</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BigData复习笔记00：系统架构与常见业务]]></title>
    <url>%2Fposts%2F92bad4a9%2F</url>
    <content type="text"><![CDATA[数据行业常用工具及常见业务介绍![stephen-dawson-qwtCeJ5cLYs-unsplash](https://image.alessa0.cn/132845.jpg)Chapter00. 软件架构与业务场景国家数据# 系统架构系统工具底层架构：分布式文件系统存储：分布式数据库计算框架：离线批量/在线实时工具：SQL封装查询/机器学习调度：分布式锁算法Lambda架构离线批量+实时处理批量计算：延时高，可使用复杂算法多次处理，结果相对精确实时计算：延时低，能够对新Item进行处理，实时推荐Lambda架构结合批量计算和实时处理，能够给出相对完整的推荐结果数据数据来源基本来自两类：PGC：专业机构UGC：用户物品(item)数据拥有不同属性（名称/描述/…），统称为元数据(metadata)用户(user)数据拥有不同行为（点击/收藏/支付…）业务场景场景1：推荐流程第一阶段（召回）：用Token检索Item/用Item检索Item，本质是找候选的过程【粗排】第二阶段（过滤）：把候选集合中劣质的Item过滤掉第三阶段（排序）：把好的Item排在前面【精排】第四阶段（截断）：取Top-N(和产品形态有关)简易版用户User点击物品ItemA，触发埋点推荐系统引擎接收到埋点信息，发出请求，在索引数据库寻找与ItemA相关的ItemB、ItemC、ItemD…，并将结果取Top-N返回推荐给User索引数据库(NoSQL)日常维护使用分词技术，将物品库中Item划分出各个特征，构建出正排表Item -&gt; TokenA, TokenB, TokenC…根据正排表特征，构建出倒排表Token -&gt; ItemA, ItemB, ItermC…方式1：将倒排表写入索引库（搜索引擎搜索时根据分词token实时计算得出相关Item）方式2：根据正排表与倒排表，构建出ItemA -&gt; ItemB, ItemC, ItermD… 写入索引库（索引库维护时根据Item分词token离线计算得出相关Item）进阶版用户User点击物品ItemA，触发埋点推荐系统引擎接收到埋点信息，发出请求，在索引数据库A寻找与ItemA相关的ItemB、ItemC、ItemD…以算法1打分排序取Top-N1，在索引数据库B寻找与ItemA相关的Item1、Item2、Item3…以算法2打分排序取Top-N2，在索引数据库C寻找与ItemA相关的Item-I、Item-II、Item-III…将不同索引库所得结果Item去除分数放入打分模型，按打分高低取出Top-N结果返回推荐给User每个索引数据库(NoSQL)日常维护使用分词技术，将物品库中Item划分出各个特征，按打分高低排序，构建出正排表Item -&gt; TokenA: ScoreA, TokenB: ScoreB, TokenC: ScoreC…根据正排表特征，按打分高低排序，构建出倒排表Token -&gt; ItemA: Score1, ItemB: Score2, ItermC: Score3…方式1：将倒排表写入索引库（搜索引擎搜索时根据分词token实时计算得出相关Item）方式2：根据正排表与倒排表，构建出ItemA -&gt; ItemB: Score-I, ItemC: Score-II, ItermD: Score-III… 写入索引库（索引库维护时根据Item分词token离线计算得出相关Item）场景2：搜索引擎常用搜索场景：人与内容（PC时代） -&gt; 人与服务（移动时代）各大门户网站利用平台优势聚合大量信息，为用户提供桥梁，“有求必应”连接用户与内容，连接用户与服务搜索引擎解决方案Demo：使用的基于MapReduce的建库系统(建库流)目的：构建供检索使用的索引和摘要输入：网页输出：索引和摘要处理方法：多轮Map-Reduce页面解析并处理页面属性输出正排表分析构建倒排表结果分析Merge合并场景3：广告系统参与者：网民/广告主/平台广告触发：Keyword Targeting广告主/网民通过关键字表达需求网民输入query与广告主购买的Keyword进行匹配CTR点击率预估：机器学习点击率用于广告排序提升搜索引擎收益保护网民利益广告排序：关键词广告拍卖每次动态广告展现都是一次动态的拍卖排序函数基本流程：产品发布，广告主向平台购买相对应关键词Token平台将产品加入广告库，同时更新广告索引库索引库维护，更新倒排表中产品排序平台根据索引库产品排序，在平台展现产品广告给网民网民主动点击广告，或根据网民搜索输入的query与广告库索引进行匹配检索，展现给网民项目结构Demo推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on Yarn移动应用架构之IO蒸馏]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>复习笔记</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>业务</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink电商团购项目（一）]]></title>
    <url>%2Fposts%2F451f3483%2F</url>
    <content type="text"><![CDATA[Flink项目环境搭建![daniel-chen-546446-unsplash](https://image.alessa0.cn/090812.jpg)第一章 大数据集群搭建Flink# 集群环境系统CentOS-7-x86_64-Minimal-1810.isoflink-master1 192.168.69.121 2core 2Gflink-master2 192.168.69.122 2core 2Gflink-slave1 192.168.69.123 1core 2Gflink-slave2 192.168.69.124 1core 2Gflink-slave3 192.168.69.125 1core 2GHadoop集群组件列表组件Master1Master2Slave1Slave2Slave3jdk1.8.0_212√√√√√scala-2.11.12√√√√√miniconda3√√√√√hadoop2.7.0√√√√√kafka0.11√√√√√mariadb/mariadb-server√√Hive2.2.0√√zookeeper 3.4.9√√√Flume1.9√√√√√Flink1.7√√√√√组件安装步骤参考往期系列文章Flink电商团购项目（零）Hadoop环境搭建推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on Yarn通行证项目前端开发小结拾贝电台开源了]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>项目实战</category>
        <category>Flink电商团购</category>
        <category>01-大数据集群搭建</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Flink</tag>
        <tag>项目</tag>
        <tag>电商</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink电商团购项目（零）：环境搭建]]></title>
    <url>%2Fposts%2F8d61cd2a%2F</url>
    <content type="text"><![CDATA[Flink项目环境搭建![janko-ferlic-214224-unsplash](https://image.alessa0.cn/045237.jpg)第一章 大数据集群搭建Flink# 集群组件安装安装JDKcd /mnt/hgfs/aboutyuncp jdk-8u212-linux-x64.tar.gz /usr/aboutyuncd /usr/aboutyuntar zxvf jdk-8u212-linux-x64.tar.gzrm -rf jdk-8u212-linux-x64.tar.gzmv jdk1.8.0_212 jdk#配置JDK环境变量vim /etc/profile1234# SET JAVA PATH export JAVA_HOME=/usr/aboutyun/jdk export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib export PATH=$PATH:$JAVA_HOME/binsource /etc/profile安装Scalacd /mnt/hgfs/aboutyuncp scala-2.11.12.tgz /usr/aboutyuncd /usr/aboutyuntar zxvf scala-2.11.12.tgzrm -rf scala-2.11.12.tgzmv scala-2.11.12 scala#配置Scala环境变量vim /etc/profile123# SET SCALA PATH export SCALA_HOME=/usr/aboutyun/scala export PATH=$PATH:$SCALA_HOME/binsource /etc/profile安装miniconda3cd /mnt/hgfs/aboutyuncp Miniconda3-latest-Linux-x86_64.sh /usr/local/src/cd /usr/local/srcsudo yum -y install bzip2sh Miniconda3-latest-Linux-x86_64.shrm -rf Miniconda3-latest-Linux-x86_64.sh#配置环境变量：source ~/.bashrc#更新conda环境：conda update –all安装Zookeeper仅在Slave节点cd /mnt/hgfs/aboutyuncp zookeeper-3.4.9.tar.gz /usr/aboutyun/cd /usr/aboutyun/tar zxvf zookeeper-3.4.9.tar.gzrm -rf zookeeper-3.4.9.tar.gzmv zookeeper-3.4.9 zookeepercd zookeeper#配置Zookeeper环境变量vim /etc/profile123# SET ZOOKEEPER PATH export ZOOKEEPER_HOME=/usr/aboutyun/zookeeper export PATH=$PATH:$ZOOKEEPER_HOME/binsource /etc/profile#修改Zookeeper配置mkdir datamkdir logscd confcp zoo_sample.cfg zoo.cfgvim zoo.cfg12345dataDir=/usr/aboutyun/zookeeper/data dataLogDir=/usr/aboutyun/zookeeper/logs server.1=flink-slave1:2888:3888 server.2=flink-slave2:2888:3888 server.3=flink-slave3:2888:3888#分别添加ID123456#Slave1 echo "1" &gt; /usr/aboutyun/zookeeper/data/myid #Slave2 echo "2" &gt; /usr/aboutyun/zookeeper/data/myid #Slave3 echo "3" &gt; /usr/aboutyun/zookeeper/data/myid#启动Zookeeper服务zkServer.sh start#查看运行状态zkServer.sh statusjps#关闭Zookeeper服务zkServer.sh stop安装Hadoopcd /mnt/hgfs/aboutyuncp hadoop-2.7.0.tar.gz /usr/aboutyun/cd /usr/aboutyun/tar zxvf hadoop-2.7.0.tar.gzrm -rf hadoop-2.7.0.tar.gzmv hadoop-2.7.0 hadoopcd hadoop#配置Hadoop环境变量vim /etc/profile12345# SET HADOOP PATH export HADOOP_HOME=/usr/aboutyun/hadoopexport PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbinexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopsource /etc/profile#创建临时目录和文件目录mkdir -p /usr/aboutyun/hadoop/dfs/namemkdir -p /usr/aboutyun/hadoop/dfs/datamkdir -p /usr/aboutyun/hadoop/tmp/dfsmkdir -p /usr/aboutyun/hadoop/journalmkdir -p /usr/aboutyun/hadoop/yarn/logs#修改Hadoop配置文件cd etc/hadoopvim hadoop-env.sh1export JAVA_HOME=/usr/aboutyun/jdkvim yarn-env.sh1export JAVA_HOME=/usr/aboutyun/jdkvim slaves123flink-slave1 flink-slave2flink-slave3vim core-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt; &lt;description&gt;默认文件系统的名称。一个URI，其方案和权限决定了FileSystem的实现。&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;flink-slave1:2181,flink-slave2:2181,flink-slave3:2181&lt;/value&gt; &lt;description&gt;由逗号分隔的ZooKeeper服务器地址列表，由ZKFailoverController在自动故障转移中使用。&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/aboutyun/hadoop/tmp&lt;/value&gt; &lt;description&gt;数据目录目录&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfenceshell(/bin/true)&lt;/value&gt; &lt;description&gt;用于服务防护的防护方法列表。可能包含内置方法（例如shell和sshfence）或用户定义的方法。&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/aboutyun/.ssh/id_rsa&lt;/value&gt; &lt;description&gt;用于内置sshfence fencer的SSH私钥文件。&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;description&gt;SequenceFiles中使用的读/写缓冲区的大小。&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ipc.client.connect.max.retries&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;description&gt;客户端为建立服务器连接而重试的次数。&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ipc.client.connect.retry.interval&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt; &lt;description&gt;客户端在重试建立服务器连接之前将等待的毫秒数。&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt;vim hdfs-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;flink-master1,flink-master2&lt;/value&gt; &lt;description&gt;给定名称服务的前缀包含给定名称服务的逗号分隔的名称节点列表。&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.flink-master1&lt;/name&gt; &lt;value&gt;flink-master1:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.flink-master2&lt;/name&gt; &lt;value&gt;flink-master2:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.flink-master1&lt;/name&gt; &lt;value&gt;flink-master1:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.flink-master2&lt;/name&gt; &lt;value&gt;flink-master2:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://flink-slave1:8485;flink-slave2:8485;flink-slave3:8485/mycluster&lt;/value&gt; &lt;description&gt;HA群集中多个名称节点之间的共享存储上的目录。此目录将由活动写入并由备用数据库读取，以保持命名空间同步。&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;description&gt;配置Java类的名称，DFS客户端将使用该名称来确定哪个NameNode是当前的Active，以及哪个NameNode当前正在为客户端请求提供服务。&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;是否启用自动故障转移。&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;如果为“true”，则启用HDFS中的权限检查。如果为“false”，则关闭权限检查，但所有其他行为都保持不变。&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/usr/aboutyun/hadoop/journal&lt;/value&gt; &lt;description&gt;指定JournalNode在本地磁盘存放数据的位置&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///usr/aboutyun/hadoop/dfs/name&lt;/value&gt; &lt;description&gt;设置namenode存放路径&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///usr/aboutyun/hadoop/dfs/data&lt;/value&gt; &lt;description&gt;设置datanode存放径路&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;268435456&lt;/value&gt; &lt;description&gt;大型文件系统的HDFS块大小为256MB。&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;description&gt;namenode的服务器线程数&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt;mv mapred-site.xml.template mapred-site.xmlvim mapred-site.xml1234567891011121314151617181920212223242526272829303132333435363738394041&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;description&gt;指定mr框架为yarn方式&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt; &lt;value&gt;512&lt;/value&gt; &lt;description&gt;每个Map任务的物理内存限制&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt; &lt;value&gt;512&lt;/value&gt; &lt;description&gt;每个Reduce任务的物理内存限制&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;0.0.0.0:10020&lt;/value&gt; &lt;description&gt;MapReduce JobHistory服务器IPC主机：端口&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;0.0.0.0:19888&lt;/value&gt; &lt;description&gt;MapReduce JobHistory服务器Web浏览时的主机：端口&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.application.classpath&lt;/name&gt; &lt;value&gt; /usr/aboutyun/hadoop/etc/hadoop, /usr/aboutyun/hadoop/share/hadoop/common/*, /usr/aboutyun/hadoop/share/hadoop/common/lib/*, /usr/aboutyun/hadoop/share/hadoop/hdfs/*, /usr/aboutyun/hadoop/share/hadoop/hdfs/lib/*, /usr/aboutyun/hadoop/share/hadoop/mapreduce/*, /usr/aboutyun/hadoop/share/hadoop/mapreduce/lib/*, /usr/aboutyun/hadoop/share/hadoop/yarn/*, /usr/aboutyun/hadoop/share/hadoop/yarn/lib/* &lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;vim yarn-site.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;启动后启用RM以恢复状态。如果为true，则必须指定yarn.resourcemanager.store.class。&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;description&gt;用作持久存储的类。&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;flink-slave1:2181,flink-slave2:2181,flink-slave3:2181&lt;/value&gt; &lt;description&gt;ZooKeeper服务的地址，多个地址使用逗号隔开&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;启用RM高可用性。启用时，（1）默认情况下，RM以待机模式启动，并在提示时转换为活动模式。（2）RM集合中的节点列在yarn.resourcemanager.ha.rm-ids中（3）如果明确指定了yarn.resourcemanager.ha.id，则每个RM的id来自yarn.resourcemanager.ha.id或者可以通过匹配yarn.resourcemanager.address。&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;description&gt;启用HA时群集中的RM节点列表。最少2个&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;flink-master1:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;flink-master2:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;mycluster-yarn-ha&lt;/value&gt; &lt;description&gt;集群HA的id，用于在ZooKeeper上创建节点，区分使用同一个ZooKeeper集群的不同Hadoop集群&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;flink-master1&lt;/value&gt; &lt;description&gt;主机名&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;flink-master2&lt;/value&gt; &lt;description&gt;主机名&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;description&gt;reducer取数据的方式是mapreduce_shuffle&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;discription&gt;每个节点可用内存,单位MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;discription&gt;每个节点可用cpu&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;512&lt;/value&gt; &lt;discription&gt;单个任务可申请最少内存，默认1024MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; &lt;discription&gt;单个任务可申请最大内存，默认8192MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-vcores&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;discription&gt;最小的cores 1 个，默认的就是一个&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-vcores&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;discription&gt;最多可分配的cores 2 个&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;discription&gt;是否开启聚合日志&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds&lt;/name&gt; &lt;value&gt;-1&lt;/value&gt; &lt;discription&gt;定义NM唤醒上载日志文件的频率。默认值为-1。默认情况下，应用程序完成后将上载日志。通过设置此配置，可以在应用程序运行时定期上载日志。可设置的最小滚动间隔秒数为3600。&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://flink-master1:19888/jobhistory/logs&lt;/value&gt; &lt;discription&gt; 配置日志服务器的地址&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;-1&lt;/value&gt; &lt;discription&gt; 在删除聚合日志之前保留多长时间。-1禁用。单位是秒&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt; &lt;value&gt;/usr/aboutyun/hadoop/yarn/logs/&lt;/value&gt; &lt;discription&gt;nodemanager存放container日志的本地路径&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt; &lt;value&gt;/tmp/logs&lt;/value&gt; &lt;discription&gt;nodemanager存放container日志的本地路径&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.am.max-attempts&lt;/name&gt; &lt;value&gt;4&lt;/value&gt; &lt;description&gt;The maximum number of application master execution attempts.&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt;cd /usr/aboutyun/hadoop/sbin#编辑 start-dfs.sh，stop-dfs.sh 脚本1234567# 在开始处 #!/usr/bin/env bash 的下面，增加以下内容：HDFS_DATANODE_USER=rootHDFS_DATANODE_SECURE_USER=hdfsHDFS_ZKFC_USER=rootHDFS_JOURNALNODE_USER=rootHDFS_NAMENODE_USER=rootHDFS_SECONDARYNAMENODE_USER=root#编辑 start-yarn.sh，stop-yarn.sh 脚本1234# 在开始处 #!/usr/bin/env bash 的下面，增加以下内容：YARN_RESOURCEMANAGER_USER=rootHADOOP_SECURE_DN_USER=yarn YARN_NODEMANAGER_USER=root#启用JournalNode集群hadoop-daemon.sh start journalnode#初始化NameNode(仅master1)hadoop namenode -format#格式化Zookeeper(仅master1)hdfs zkfc -formatZK#启动NameNode(仅master1)hadoop-daemon.sh start namenode#将 NameNode 数据复制到备用 NameNode(仅master2)hdfs namenode -bootstrapStandbyhadoop-daemon.sh start namenode#启动Hadoop集群start-dfs.sh (仅master1)start-yarn.sh (仅master1)yarn-daemon.sh start resourcemanager(仅master2)#监控页面HDFS(master1)HDFS(master2)YARN(master1)YARN(master2)安装mysql仅在Master节点#安装mysqlyum -y install mariadb-server mariadbrpm -q mariadb mariadb-server#设置mysql开机启动systemctl enable mariadbsystemctl daemon-reload#开启mysqlsystemctl start mariadb#关闭mysqlsystemctl stop mariadb#重启mysqlsystemctl restart mariadb#查看mysql状态systemctl status mariadb#通过内置的安全脚本实现对数据库的安全保护mysql_secure_installation#登录root账户mysql -uroot -p#创建账户CREATE DATABASE metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON metastore.* TO ‘hive’@’%’ IDENTIFIED BY ‘123’;#刷新权限flush privileges;安装Hive仅在Master节点cd /mnt/hgfs/aboutyuncp apache-hive-2.2.0-bin.tar.gz /usr/aboutyun/cd /usr/aboutyun/tar zxvf apache-hive-2.2.0-bin.tar.gzrm -rf apache-hive-2.2.0-bin.tar.gzmv apache-hive-2.2.0-bin hive#配置Hive环境变量vim /etc/profile123# SET Hive PATH export HIVE_HOME=/usr/aboutyun/hive export PATH=$PATH:$HIVE_HOME/binsource /etc/profile#配置mysql驱动包cd /mnt/hgfs/aboutyuncp mysql-connector-java-5.1.47.tar.gz /usr/aboutyun/cd /usr/aboutyun/tar zxvf mysql-connector-java-5.1.47.tar.gzrm -rf mysql-connector-java-5.1.47.tar.gzcp mysql-connector-java-5.1.47-bin.jar /usr/aboutyun/hive/lib/#更换jline包（版本不一致）cp hive/lib/jline-2.12.jar /usr/aboutyun/hadoop/share/hadoop/yarn/lib/#配置hivecd hivemkdir -p data/hive/logmkdir -p data/hive/tmpmkdir -p data/hive/warehousecd confcp hive-env.sh.template hive-env.shvim hive-env.sh12345export JAVA_HOME=/usr/aboutyun/jdk export HADOOP_HOME=/usr/aboutyun/hadoop export HIVE_HOME=/usr/aboutyun/hive export HIVE_CONF_DIR=/usr/aboutyun/hive/conf export HIVE_AUX_JARS=/usr/aboutyun/hive/libcp hive-default.xml.template hive-site.xmlvim hive-site.xml123456789101112131415161718192021222324252627282930&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/usr/aboutyun/hive/data/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/usr/aboutyun/hive/data/hive/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/usr/aboutyun/hive/data/hive/log&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;把{system:java.io.tmpdir} 改成 /usr/aboutyun/hive/data/hive/tmp1:%s/$&#123;system:java.io.tmpdir&#125;/\/usr\/aboutyun\/hive\/data\/hive\/tmp/g把 {system:user.name} 改成 {user.name}1:%s/$&#123;system:user.name&#125;/aboutyun/g#初始化hive(MYSQL版)schematool -dbType mysql -initSchema安装Flumecd /mnt/hgfs/aboutyuncp apache-flume-1.9.0-bin.tar.gz /usr/aboutyun/cd /usr/aboutyun/tar zxvf apache-flume-1.9.0-bin.tar.gzrm -rf apache-flume-1.9.0-bin.tar.gzmv apache-flume-1.9.0-bin flumecd flume#Hbase环境变量vim /etc/profile123# SET FLUME PATH export FLUME_HOME=/usr/aboutyun/flume export PATH=$PATH:$FLUME_HOME/binsource /etc/profile#修改Flume配置cd confcp flume-env.sh.template flume-env.shvim flume-env.sh1export JAVA_HOME=/usr/aboutyun/jdk#验证Server12345678# NetCat flume-ng agent --conf conf --conf-file conf/flume-netcat.conf --name=agent -Dflume.root.logger=INFO,console# Exec flume-ng agent --conf conf --conf-file conf/flume-exec.conf --name=agent -Dflume.root.logger=INFO,console# Avro flume-ng agent --conf conf --conf-file conf/flume-netcat.conf --name=agent -Dflume.root.logger=DEBUG,consoleClient12345678# NetCat flume-ng agent --conf conf --conf-file conf/flume-netcat.conf --name=agent -Dflume.root.logger=INFO,console# Exec while true;do echo `date` &gt;&gt; /data/hadoop/flume/test.txt ; sleep 1; done# Avro telnet master 44444安装Kafkacd /mnt/hgfs/aboutyuncp kafka_2.11-0.11.0.3.tgz /usr/aboutyuncd /usr/aboutyuntar zxvf kafka_2.11-0.11.0.3.tgzrm -rf kafka_2.11-0.11.0.3.tgzmv kafka_2.11-0.11.0.3 kafkacd kafka#Kafka环境变量vim /etc/profile123# SET KAFKA PATH export KAFKA_HOME=/usr/aboutyun/kafka export PATH=$PATH:$KAFKA_HOME/binsource /etc/profile#修改Kafka配置mkdir logscd configvim server.properties12345678910111213log.dirs=/usr/aboutyun/kafka/logs zookeeper.connect=flink-slave1:2181,flink-slave2:2181,flink-slave3:2181 #Master1 broker.id=0#Master2 broker.id=1#Slave1 broker.id=2 #Slave2 broker.id=3#Slave3 broker.id=4普通启动：kafka-server-start.sh -daemon /usr/aboutyun/kafka/config/server.properties关闭集群：kafka-server-stop.sh安装Flinkcd /mnt/hgfs/aboutyuncp flink-1.7.2-bin-hadoop27-scala_2.11.tgz /usr/aboutyuncd /usr/aboutyuntar zxvf flink-1.7.2-bin-hadoop27-scala_2.11.tgzrm -rf flink-1.7.2-bin-hadoop27-scala_2.11.tgzmv flink-1.7.2 flinkcd flink#Flink环境变量vim /etc/profile123# SET FLINK PATH export FLINK_HOME=/usr/aboutyun/flink export PATH=$PATH:$FLINK_HOME/binsource /etc/profile#修改Flink配置cd confvim flink-conf.yaml12345678# 修改如下内容jobmanager.rpc.address: flink-master1high-availability: zookeeperhigh-availability.zookeeper.path.root: /flinkhigh-availability.cluster-id: flinkhigh-availability.storageDir: hdfs:///flink/ha/high-availability.zookeeper.quorum: flink-slave1:2181,flink-slave2:2181,flink-slave3:2181yarn.application-attempts: 10vim masters12flink-master1:8081flink-master2:8081vim slaves123flink-slave1 flink-slave2flink-slave3#启动Standalone集群HAstart-cluster.sh#关闭Standalone集群stop-cluster.sh#启动YARN集群HAyarn-session.sh#监控网页Flink(master1)推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on Yarn通行证项目前端开发小结拾贝电台开源了]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>项目实战</category>
        <category>Flink电商团购</category>
        <category>01-大数据集群搭建</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Flink</tag>
        <tag>项目</tag>
        <tag>电商</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[傻瓜式CDH集群部署指南]]></title>
    <url>%2Fposts%2F5578f504%2F</url>
    <content type="text"><![CDATA[CDH环境配置![clay-banks-1554989-unsplash](https://image.alessa0.cn/074403.jpg)单机内存最低8GCloudera# 集群环境系统CentOS-7-x86_64-Minimal-1810.isocdh-master 192.168.69.111 4core 16Gcdh-slave1 192.168.69.112 4core 8Gcdh-slave2 192.168.69.113 4core 8G关闭防火墙及修改hosts永久关闭内核防火墙vim /etc/selinux/config12# 修改如下信息SELINUX=disabled关闭系统防火墙停止firewallsystemctl stop firewalld.service禁止firewall开机启动systemctl disable firewalld.service修改hosts文件vim /etc/hosts1234# 添加如下信息192.168.69.111 cdh-master 192.168.69.112 cdh-slave1 192.168.69.113 cdh-slave2SSH互信生成密钥对（公钥和私钥）ssh-keygen -t rsa -P ‘’追加authorized_keys1234567# 追加authorized_keyscat ~/.ssh/id_rsa.pub &gt; ~/.ssh/authorized_keys# 修改权限chmod g-w ~chmod 700 ~/.ssh chmod 600 ~/.ssh/authorized_keys追加密钥到Masterssh cdh-slave1 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keysssh cdh-slave2 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys复制密钥到从节点scp /.ssh/authorized_keys cdh-slave1:/.ssh/authorized_keysscp /.ssh/authorized_keys cdh-slave2:/.ssh/authorized_keysntp时间同步所有节点安装相关ntp组件yum -y install ntp所有节点设置时区timedatectl set-timezone Asia/Shanghai启动ntp，以及设置开机启动12345# 启动ntpsystemctl start ntpd# 设置开机启动systemctl enable ntpd配置ntp服务器(master节点)vim /etc/ntp.conf12345678910# 修改如下几行#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst# 添加如下几行restrict 192.168.69.2 mask 255.255.255.0 nomodify notrapserver 127.127.1.0fudge 127.127.1.0 stratum 10配置ntp服务器(slave节点)vim /etc/ntp.conf123456789# 修改如下几行#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst# 添加如下几行(master节点)restrict 192.168.69.2 mask 255.255.255.0 nomodify notrapserver 192.168.69.111重启ntp服务systemctl restart ntpd主节点定时服务crontab -e10-59/10 * * * * /usr/sbin/ntpdate -u asia.pool.ntp.org手动同步master的时间ntpdate -u 192.168.69.111查看同步状态ntpstat配置Cloudera rpm仓库下载repo文件sudo wget https://archive.cloudera.com/cm6/6.2.0/redhat7/yum/cloudera-manager.repo -P /etc/yum.repos.d/使用GPG key导入仓库sudo rpm –import https://archive.cloudera.com/cm6/6.2.0/redhat7/yum/RPM-GPG-KEY-cloudera下载parcel文件mkdir -p /opt/cloudera/parcel-repo/sudo wget https://archive.cloudera.com/cdh6/6.2.0/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel /opt/cloudera/parcel-repo/sudo wget https://archive.cloudera.com/cdh6/6.2.0/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373-el7.parcel.sha256 /opt/cloudera/parcel-repo/sudo wget https://archive.cloudera.com/cdh6/6.2.0/parcels/manifest.json /opt/cloudera/parcel-repo/安装 Java（64-bit）卸载掉自带的 OpenJdkrpm -qa | grep java使用Cloudera Manager 安装 Javasudo yum -y install oracle-j2sdk1.8设置环境变量vim /etc/profile1234# SET JAVA PATH export JAVA_HOME=/usr/java/jdk1.8.0_181-cloudera/export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib export PATH=$PATH:$JAVA_HOME/binsource /etc/profileyum update报错Error: Delta RPMs disabled because /usr/bin/applydeltarpm not installed.123# 安装deltarpmyum provides '*/applydeltarpm'yum install deltarpm安装Cloudera Manager Server安装Cloudera Manager包sudo yum -y install cloudera-manager-daemons cloudera-manager-agent cloudera-manager-serverPs：可以在https://archive.cloudera.com/cm6/6.2.0/redhat7/yum/RPMS/x86_64/ 提前下好rpm包进行安装。启用Auto-TLS待续安装和配置数据库MariaDB for Cloudera Software(master节点)安装MariaDBsudo yum -y install mariadb-server配置MariaDBvim /etc/my.cnf1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.socktransaction-isolation = READ-COMMITTED# Disabling symbolic-links is recommended to prevent assorted security risks;# to do so, uncomment this line:symbolic-links = 0# Settings user and group are ignored when systemd is used.# If you need to run mysqld under a different user or group,# customize your systemd unit file for mariadb according to the# instructions in http://fedoraproject.org/wiki/Systemdkey_buffer = 16Mkey_buffer_size = 32Mmax_allowed_packet = 32Mthread_stack = 256Kthread_cache_size = 64query_cache_limit = 8Mquery_cache_size = 64Mquery_cache_type = 1max_connections = 550#expire_logs_days = 10#max_binlog_size = 100M#log_bin should be on a disk with enough free space.#Replace '/var/lib/mysql/mysql_binary_log' with an appropriate path for your#system and chown the specified folder to the mysql user.log_bin=/var/lib/mysql/mysql_binary_log#In later versions of MariaDB, if you enable the binary log and do not set#a server_id, MariaDB will not start. The server_id must be unique within#the replicating group.server_id=1binlog_format = mixedread_buffer_size = 2Mread_rnd_buffer_size = 16Msort_buffer_size = 8Mjoin_buffer_size = 8M# InnoDB settingsinnodb_file_per_table = 1innodb_flush_log_at_trx_commit = 2innodb_log_buffer_size = 64Minnodb_buffer_pool_size = 4Ginnodb_thread_concurrency = 8innodb_flush_method = O_DIRECTinnodb_log_file_size = 512M[mysqld_safe]log-error=/var/log/mariadb/mariadb.logpid-file=/var/run/mariadb/mariadb.pid## include all files from the config directory#!includedir /etc/my.cnf.d启动MariaDB开机启动MariaDBsudo systemctl enable mariadb启动MariaDB服务sudo systemctl start mariadb安全设置sudo /usr/bin/mysql_secure_installation1234567891011121314151617181920[...]Enter current password for root (enter for none):OK, successfully used password, moving on...[...]Set root password? [Y/n] YNew password: 123Re-enter new password: 123[...]Remove anonymous users? [Y/n] Y[...]Disallow root login remotely? [Y/n] Y[...]Remove test database and access to it [Y/n] Y[...]Reload privilege tables now? [Y/n] Y[...]All done! If you've completed all of the above steps, your MariaDBinstallation should now be secure.Thanks for using MariaDB!安装MySQL JDBC Driver for MariaDB下载驱动包wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.46.tar.gz解压驱动包tar zxvf mysql-connector-java-5.1.46.tar.gzrm -rf mysql-connector-java-5.1.46.tar.gz复制驱动包sudo mkdir -p /usr/share/java/cd mysql-connector-java-5.1.46sudo cp mysql-connector-java-5.1.46-bin.jar /usr/share/java/mysql-connector-java.jar创建数据库登录root账户mysql -uroot -p创建hive数据库CREATE DATABASE metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;CREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;创建账户GRANT ALL ON metastore.* TO ‘hive’@’%’ IDENTIFIED BY ‘123’;GRANT ALL ON scm.* TO ‘scm’@’%’ IDENTIFIED BY ‘123’;GRANT ALL ON oozie.* TO ‘oozie’@’%’ IDENTIFIED BY ‘123’;GRANT ALL ON hue.* TO ‘hue’@’%’ IDENTIFIED BY ‘123’;刷新权限flush privileges;查看数据库SHOW DATABASES;查看权限SHOW GRANTS FOR ‘hive’@’%’;设置Cloudera Manager数据库(master节点)设置语法sudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm 123准备数据库sudo /opt/cloudera/cm/schema/scm_prepare_database.sh mysql scm scm安装CDH及其他软件【强烈建议】所有节点拍摄快照启动Cloudera Manager Server(master节点)sudo systemctl start cloudera-scm-server查看日志(master节点)sudo tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log直至出现INFO WebServerImpl:com.cloudera.server.cmf.WebServerImpl: Started Jetty server.登录web页面http://cdh-master:7180账户：admin密码：admin安装组件- [x] 接受最终用户许可条款和条件选择Cloudera Express免费版集群安装Cluster BasicsCDHSpecify Hosts输入cdh-master, cdh-slave1, cdh-slave2点击’搜索’选择存储库选择Public Cloudera Repository选择Parcels其他默认JDK 安装选项选中安装 Oracle Java SE 开发工具包 (JDK)设置登录凭据选择root输入密码安装agents安装ParcelsInspect Hosts虚拟内存设置12sysctl -w vm.swappiness=10echo vm.swappiness = 10 &gt;&gt; /etc/sysctl.conf大内存页设置1234567891011# 临时echo never&gt;/sys/kernel/mm/transparent_hugepage/defragecho never&gt;/sys/kernel/mm/transparent_hugepage/enabled# 永久vim /etc/rc.local# 加入如下信息echo never&gt;/sys/kernel/mm/transparent_hugepage/defragecho never&gt;/sys/kernel/mm/transparent_hugepage/enabled# 设置权限chmod +x /etc/rc.d/rc.local组件列表组件版本Supervisord3.0Cloudera Manager Agent6.2.0Cloudera Manager Management Daemon6.2.0Flume NG1.9.0+cdh6.2.0Hadoop3.0.0+cdh6.2.0HDFS3.0.0+cdh6.2.0HttpFS3.0.0+cdh6.2.0hadoop-kms3.0.0+cdh6.2.0MapReduce 23.0.0+cdh6.2.0YARN3.0.0+cdh6.2.0HBase2.1.0+cdh6.2.0Lily HBase Indexer1.5+cdh6.2.0Hive2.1.1+cdh6.2.0HCatalog2.1.1+cdh6.2.0Hue4.2.0+cdh6.2.0Impala3.2.0+cdh6.2.0Java 81.8.0_181Kafka2.1.0+cdh6.2.0Kite（仅限 CDH 5 ）1.0.0+cdh6.2.0kudu1.9.0+cdh6.2.0Oozie5.1.0+cdh6.2.0Parquet1.9.0+cdh6.2.0Pig0.17.0+cdh6.2.0sentry2.1.0+cdh6.2.0Solr7.4.0+cdh6.2.0spark2.4.0+cdh6.2.0Sqoop1.4.7+cdh6.2.0ZooKeeper3.4.5+cdh6.2.0使用向导设置群集选择服务选择所有服务选择HBase HDFS Hive Hue Kafka Oozie YARN(MR2 Included) ZooKeeper自定义角色分配默认数据库设置HiveMySQL &gt; 否 &gt; cdh-master &gt; metastore &gt; hive &gt; 123OozieMySQL &gt; cdh-master &gt; oozie &gt; oozie &gt; 123HueMySQL &gt; cdh-master &gt; hue &gt; hue &gt; 123审核更改默认命令详细信息Ps：若搭建过程中多次尝试安装，建议删除/dfs/nn下文件，避免HDFS服务报错汇总错误排查推荐链接 &gt;&gt; https://blog.csdn.net/zzq900503/article/details/53393721推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on Yarn]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>CDH环境搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 集群搭建(十五)：Flink]]></title>
    <url>%2Fposts%2F6cff886e%2F</url>
    <content type="text"><![CDATA[Flink：Storm你该回家了…![shane-young-769905-unsplash](https://image.alessa0.cn/090827.jpg)Flink 安装配置Flink# 安装Flinkcd /mnt/hgfs/Hadoopcp flink-1.8.0-bin-scala_2.11.tgz /usr/local/src/cd /usr/local/src/tar zxvf flink-1.8.0-bin-scala_2.11.tgzrm -rf flink-1.8.0-bin-scala_2.11.tgzFlink环境变量：vim ~/.bashrc1234# 添加如下信息# SET FLINK PATH export FLINK_HOME=/usr/local/src/flink-1.8.0 export PATH=$PATH:$FLINK_HOME/binsource ~/.bashrc修改Flink配置cd flink-1.8.0/confvim flink-conf.yaml12# 修改如下信息jobmanager.rpc.address: mastervim masters12# 修改如下信息master:8081vim slaves12slave1 slave2启动集群启动服务：start-cluster.sh关闭集群：stop-cluster.sh监控网页：Web：http://master:8081推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on Yarn]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>环境搭建</category>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Flink</tag>
        <tag>Hadoop环境搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 集群搭建(十四)：Storm on Yarn]]></title>
    <url>%2Fposts%2F6a54df13%2F</url>
    <content type="text"><![CDATA[Storm-on-Yarn：跟着老大哥有肉吃…![max-delsid-479103-unsplash](https://image.alessa0.cn/090019.jpg)Storm-on-Yarn 安装配置Storm on Yarn# 待续……推荐文章:Hadoop 集群搭建(八)：SparkBigData复习笔记03：推荐算法]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>环境搭建</category>
        <category>Storm-on-Yarn</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hadoop环境搭建</tag>
        <tag>Storm-on-Yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 集群搭建(十三)：Storm]]></title>
    <url>%2Fposts%2F9017301c%2F</url>
    <content type="text"><![CDATA[Storm：我才是流处理！![johannes-plenio-247177-unsplash](https://image.alessa0.cn/084537.jpg)Storm 安装配置Storm# 安装Stormcd /mnt/hgfs/Hadoopcp apache-storm-1.2.2.tar.gz /usr/local/src/cd /usr/local/src/tar zxvf apache-storm-1.2.2.tar.gzrm -rf apache-storm-1.2.2.tar.gzStorm环境变量：vim ~/.bashrc123# SET STORM PATH export STORM_HOME=/usr/local/src/apache-storm-1.2.2 export PATH=$PATH:$STORM_HOME/binsource ~/.bashrc修改Storm配置文件cd apache-storm-1.2.2创建日志文件/数据文件：mkdir datamkdir logs配置文件：cd confvim storm.yaml12345678910111213141516# 添加如下信息storm.zookeeper.servers: - "master" - "slave1" - "slave2" storm.zookeeper.port: 2181 storm.local.dir: "/usr/local/storm-1.2.2/data" ui.port: 8089 nimbus.seeds: ["master"] supervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 - 6704 - 6705启动Storm集群主节点启动：storm nimbus &gt;&gt; /usr/local/src/apache-storm-1.2.2/logs/nimbus.out 2&gt;&amp;1 &amp;storm ui &gt;&gt; /usr/local/src/apache-storm-1.2.2/logs/ui.out 2&gt;&amp;1 &amp;从节点启动：storm supervisor &gt;&gt; /usr/local/src/apache-storm-1.2.2/logs/supervisor.out 2&gt;&amp;1 &amp;storm logviewer &gt;&gt; /usr/local/src/apache-storm-1.2.2/logs/logviewer.out 2&gt;&amp;1 &amp;主节点关闭：kill -9 `ps -ef | grep ui.core | awk ‘{print $2}’ | head -n 1`kill -9 `ps -ef | grep daemon.nimbus | awk ‘{print $2}’ | head -n 1`kill -9 `ps -ef | grep daemon.supervisor | awk ‘{print $2}’ | head -n 1`kill -9 `ps -ef | grep daemon.logviewer | awk ‘{print $2}’ | head -n 1`WEB监控页面：Web： http://master:8089/index.html推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on Yarn]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>环境搭建</category>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hadoop环境搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 集群搭建(十二)：Kafka]]></title>
    <url>%2Fposts%2Feee41037%2F</url>
    <content type="text"><![CDATA[Kafka：我不是写小说那个…![title-page](https://image.alessa0.cn/082116.jpg)Kafka 安装配置Kafka# 安装Kafkacd /mnt/hgfs/Hadoopcp kafka_2.11-2.2.0.tgz /usr/local/src/cd /usr/local/src/tar zxvf kafka_2.11-2.2.0.tgzrm -rf kafka_2.11-2.2.0.tgzKafka环境变量：vim ~/.bashrc1234# 添加如下信息# SET KAFKA PATH export KAFKA_HOME=/usr/local/src/kafka_2.11-2.2.0 export PATH=$PATH:$KAFKA_HOME/binsource ~/.bashrc修改Kafka配置cd kafka_2.11-2.2.0创建日志文件：mkdir logs配置文件：cd configvim server.properties123# 添加如下信息log.dirs=/usr/local/src/kafka_2.11-2.2.0/logs zookeeper.connect=master:2181,slave1:2181,slave2:2181仅在Masterbroker.id=0仅在Slave1broker.id=1仅在Slave2broker.id=2启动Kafka集群普通启动：kafka-server-start.sh -daemon /usr/local/src/kafka_2.11-2.2.0/config/server.properties关闭集群：kafka-server-stop.sh推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on Yarn]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>环境搭建</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hadoop环境搭建</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 集群搭建(十一)：Flume]]></title>
    <url>%2Fposts%2F41d58b55%2F</url>
    <content type="text"><![CDATA[Flume：xx托我给您带个话…![clark-young-160446-unsplash](https://image.alessa0.cn/072006.jpg)Flume 安装配置Flume# 安装Flumecd /mnt/hgfs/Hadoopcp apache-flume-1.9.0-bin.tar.gz /usr/local/src/cd /usr/local/src/tar zxvf apache-flume-1.9.0-bin.tar.gzrm -rf apache-flume-1.9.0-bin.tar.gzHbase环境变量：vim ~/.bashrc1234# 添加如下信息# SET FLUME PATH export FLUME_HOME=/usr/local/src/apache-flume-1.9.0-bin export PATH=$PATH:$FLUME_HOME/binsource ~/.bashrc修改Flume配置cd apache-flume-1.9.0-bin/confcp flume-env.sh.template flume-env.shvim flume-env.sh12# 添加如下信息export JAVA_HOME=/usr/local/src/jdk1.8.0_212新增配置文件NetCat：vim flume-netcat.conf12345678910111213141516171819202122# 添加如下信息# Name the components on this agentagent.sources = r1agent.sinks = k1agent.channels = c1# Describe/configuration the sourceagent.sources.r1.type = netcatagent.sources.r1.bind = 127.0.0.1agent.sources.r1.port = 44444# Describe the sinkagent.sinks.k1.type = logger# Use a channel which buffers events in memoryagent.channels.c1.type = memoryagent.channels.c1.capacity = 1000agent.channels.c1.transactionCapacity = 100# Bind the source and sink to the channelagent.sources.r1.channels = c1agent.sinks.k1.channel = c1Exec：vim flume-exec.conf123456789101112131415161718192021# 添加如下信息# Name the components on this agentagent.sources = r1agent.sinks = k1agent.channels = c1# Describe/configuration the sourceagent.sources.r1.type = execagent.sources.r1.command = tail -f /data/hadoop/flume/test.txt# Describe the sinkagent.sinks.k1.type = logger# Use a channel which buffers events in memoryagent.channels.c1.type = memoryagent.channels.c1.capacity = 1000agent.channels.c1.transactionCapacity = 100# Bind the source and sink to the channelagent.sources.r1.channels = c1agent.sinks.k1.channel = c1Avro：vim flume-avro.conf123456789101112131415161718192021222324# 添加如下信息# Define a memory channel called c1 on agentagent.channels.c1.type = memory# Define an avro source alled r1 on agent and tell itagent.sources.r1.channels = c1agent.sources.r1.type = avroagent.sources.r1.bind = 127.0.0.1agent.sources.r1.port = 44444# Describe/configuration the sourceagent.sinks.k1.type = hdfsagent.sinks.k1.channel = c1agent.sinks.k1.hdfs.path = hdfs://master:9000/flume_data_poolagent.sinks.k1.hdfs.filePrefix = events-agent.sinks.k1.hdfs.fileType = DataStreamagent.sinks.k1.hdfs.writeFormat = Textagent.sinks.k1.hdfs.rollSize = 0agent.sinks.k1.hdfs.rollCount= 600000agent.sinks.k1.hdfs.rollInterval = 600agent.channels = c1agent.sources = r1agent.sinks = k1验证NetCat：# 服务端flume-ng agent –conf conf –conf-file conf/flume-netcat.conf –name=agent -Dflume.root.logger=INFO,console# 客户端flume-ng agent –conf conf –conf-file conf/flume-netcat.conf –name=agent -Dflume.root.logger=INFO,consoleExec：# 服务端flume-ng agent –conf conf –conf-file conf/flume-exec.conf –name=agent -Dflume.root.logger=INFO,console# 客户端Avro：# 服务端flume-ng agent –conf conf –conf-file conf/flume-netcat.conf –name=agent -Dflume.root.logger=DEBUG,console# 客户端telnet master 44444推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on Yarn]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>环境搭建</category>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hadoop环境搭建</tag>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 集群搭建(十)：Thrift]]></title>
    <url>%2Fposts%2F34d202af%2F</url>
    <content type="text"><![CDATA[Thrift：行李少的可以从我这儿走…![alejandro-escamilla-10-unsplash](https://image.alessa0.cn/035105.jpg)Thrift 安装配置Thrift# 安装Thrift 仅在Master安装依赖环境：yum -y install automake libtool flex bison pkgconfig gcc-c++ boost-devel libevent-devel zlib-devel python-devel ruby-devel openssl-develyum -y install boost-develyum -y install libevent-devel安装：cd /mnt/hgfs/Hadoopcp thrift-0.12.0.tar.gz /usr/local/src/cd /usr/local/src/tar zxvf thrift-0.12.0.tar.gzrm -rf thrift-0.12.0.tar.gzc++三部曲：cd thrift-0.12.0./configure –with-cpp=no –with-ruby=nomakemake installHbase源码包cd /mnt/hgfs/Hadoopcp hbase-1.3.3-src.tar.gz /usr/local/src/tmpcd /usr/local/src/tmptar zxvf hbase-1.3.3-src.tar.gzmv hbase-1.3.3 hbase-1.3.3-srcrm -rf hbase-1.3.3-src.tar.gzmv hbase-1.3.3-src/ ../待续……推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on Yarn]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>环境搭建</category>
        <category>Thrift</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hadoop环境搭建</tag>
        <tag>Thrift</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 集群搭建(九)：Hbase]]></title>
    <url>%2Fposts%2F497b75db%2F</url>
    <content type="text"><![CDATA[Hbase：我不认识Hive…![campaign-creators-771723-unsplash](https://image.alessa0.cn/063435.jpg)Hbase 安装配置Hbase# 安装Hbasecd /mnt/hgfs/Hadoopcp hbase-1.3.3-bin.tar.gz /usr/local/src/cd /usr/local/src/tar zxvf hbase-1.3.3-bin.tar.gzrm -rf hbase-1.3.3-bin.tar.gzHbase环境变量：vim ~/.bashrc123456# 添加如下信息# SET HBASE PATH export HBASE_HOME=/usr/local/src/hbase-1.3.3 export HBASE_CLASSPATH=$HBASE_HOME/conf export HBASE_LOG_DIR=$HBASE_HOME/logs export PATH=$PATH:$HBASE_HOME/binsource ~/.bashrc修改Hbase配置cd hbase-1.3.3创建临时目录和文件目录：mkdir logsmkdir tmp配置文件：cd confvim regionservers123# 添加如下信息slave1 slave2vim hbase-env.sh12345# 添加如下信息export JAVA_HOME=/usr/local/src/jdk1.8.0_212 export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib # 禁用Hbase自带独立Zookeeper集群export HBASE_MANAGES_ZK=falsevim hbase-site.xml12345678910111213141516171819202122232425262728# 添加如下信息&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/src/hbase-1.3.3/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;master,slave1,slave2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/usr/local/src/zookeeper-3.4.14&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master.info.port&lt;/name&gt; &lt;value&gt;60010&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;启动集群仅在Master启动Hbase服务：start-hbase.sh关闭Hbase服务：stop-hbase.sh推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on Yarn]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>环境搭建</category>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hadoop环境搭建</tag>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 集群搭建(八)：Spark]]></title>
    <url>%2Fposts%2F55aa9f3f%2F</url>
    <content type="text"><![CDATA[Spark：传说中的星二代![aziz-acharki-370112-unsplash](https://image.alessa0.cn/052725.jpg)Spark 安装配置Spark# 安装Sparkcd /mnt/hgfs/Hadoopcp spark-2.3.3-bin-hadoop2.7.tgz /usr/local/src/cd /usr/local/src/tar zxvf spark-2.3.3-bin-hadoop2.7.tgzrm -rf spark-2.3.3-bin-hadoop2.7.tgz配置Spark环境变量：vim ~/.bashrc1234# 添加如下信息# SET SPARK PATH export SPARK_HOME=/usr/local/src/spark-2.3.3-bin-hadoop2.7 export PATH=$PATH:$SPARK_HOME/binsource ~/.bashrc修改spark配置文件cd spark-2.3.3-bin-hadoop2.7/confcp spark-env.sh.template spark-env.shvim spark-env.sh123456789# 添加如下信息export SCALA_HOME=/usr/local/src/scala-2.11.12 export JAVA_HOME=/usr/local/src/jdk1.8.0_212 export HADOOP_HOME=/usr/local/src/hadoop-2.8.5 export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181" SPARK_MASTER_IP=master SPARK_LOCAL_DIRS=/usr/local/src/spark-2.3.3-bin-hadoop2.7 SPARK_DRIVER_MEMORY=1Gcp slaves.template slavesvim slaves123# 添加如下信息slave1slave2启动Standalone仅在Master启动集群：cd spark-2.3.3-bin-hadoop2.7/sbin./start-all.shWEB监控页面：Spark：http://master:8080验证cd spark-2.3.3-bin-hadoop2.7本地模式：./bin/run-example SparkPi 10 –master local[2]集群Standlone：./bin/spark-submit –class org.apache.spark.examples.SparkPi –master spark://master:7077 examples/jars/spark-examples_2.11-2.3.3.jar 10Spark on yarn：./bin/spark-submit –class org.apache.spark.examples.SparkPi –master yarn-cluster examples/jars/spark-examples_2.11-2.3.3.jar 10推荐文章:Hadoop 集群搭建(十四)：Storm on YarnBigData复习笔记03：推荐算法]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>环境搭建</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hadoop环境搭建</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 集群搭建(七)：Hive]]></title>
    <url>%2Fposts%2F9cf82033%2F</url>
    <content type="text"><![CDATA[Hive：不要把我认成Hbase![mounzer-awad-348688-unsplash](https://image.alessa0.cn/035137.jpg)Hive 安装配置Hive# 安装Hive 仅在Master 仅在Clientcd /mnt/hgfs/Hadoopcp apache-hive-2.3.4-bin.tar.gz /usr/local/src/cd /usr/local/src/tar zxvf apache-hive-2.3.4-bin.tar.gzrm -rf apache-hive-2.3.4-bin.tar.gz配置Hive环境变量：vim ~/.bashrc1234# 添加如下信息# SET Hive PATH export HIVE_HOME=/usr/local/src/apache-hive-2.3.4-bin export PATH=$PATH:$HIVE_HOME/binsource ~/.bashrc配置mysql驱动包仅在Mastercd /mnt/hgfs/Hadoopcp mysql-connector-java-5.1.47.tar.gz /usr/local/src/cd /usr/local/src/tar zxvf mysql-connector-java-5.1.47.tar.gzrm -rf mysql-connector-java-5.1.47.tar.gzcp mysql-connector-java-5.1.47-bin.jar /usr/local/src/apache-hive-2.3.4-bin/lib/更换jline包（版本不一致）：cp apache-hive-2.3.4-bin/lib/jline-2.12.jar /usr/local/src/hadoop-2.8.5/share/hadoop/yarn/lib/配置hive仅在Master 仅在Clientcd apache-hive-2.3.4-bin创建临时目录/日志目录/数仓目录：mkdir -p data/hive/logmkdir -p data/hive/tmpmkdir -p data/hive/warehouse配置文件：cd confcp hive-env.sh.template hive-env.shvim hive-env.sh123456# 添加如下信息export JAVA_HOME=/usr/local/src/jdk1.8.0_212 export HADOOP_HOME=/usr/local/src/hadoop-2.8.5 export HIVE_HOME=/usr/local/src/apache-hive-2.3.4-bin export HIVE_CONF_DIR=/usr/local/src/apache-hive-2.3.4-bin/conf export HIVE_AUX_JARS=/usr/local/src/apache-hive-2.3.4-bin/libcp hive-default.xml.template hive-site.xmlvim hive-site.xml1234567891011121314151617181920212223242526272829303132# 添加如下信息&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://master:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;alessa0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;&#123;密码&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/usr/local/src/apache-hive-2.3.4-bin/data/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/usr/local/src/apache-hive-2.3.4-bin/data/hive/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/usr/local/src/apache-hive-2.3.4-bin/data/hive/log&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;把{system:java.io.tmpdir} 改成 /usr/local/src/apache-hive-2.3.4-bin/data/hive/tmp：1:%s/$&#123;system:java.io.tmpdir&#125;/\/usr\/local\/src\/apache-hive-2.3.4-bin\/data\/hive\/tmp/g把 {system:user.name} 改成 {user.name} ：1:%s/$&#123;system:user.name&#125;/alessa0/g初始化hive(MySQL版)schematool -dbType mysql -initSchema配置使用hiveserver2仅在Mastervim hive-site.xml12345678910111213141516171819202122232425262728293031323334# 添加如下信息 &lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://master:9083&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.support.concurrency&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt; &lt;value&gt;master:2181,slave1:2181,slave2:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.webui.host&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.webui.port&lt;/name&gt; &lt;value&gt;10002&lt;/value&gt; &lt;/property&gt;服务端启动仅在Master启动 metastore 服务nohup hive –service metastore &gt;&gt; /usr/local/src/apache-hive-2.3.4-bin/logs/hivelog.log 2&gt;&amp;1 &amp;启动 hiveserver2 服务nohup hiveserver2 1&gt;/usr/local/src/apache-hive-2.3.4-bin/logs/hiveserver.log 2&gt;/usr/local/src/apache-hive-2.3.4-bin/logs/hiveserver.err &amp;测试Web UI：http://master:10002/客户端连接仅在Client启动beeline ：beeline -u “jdbc:hive2://master:10000” alessa0 1008退出beeline ：!q推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on Yarn]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>环境搭建</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hadoop环境搭建</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 集群搭建(六)：MySQL]]></title>
    <url>%2Fposts%2F3f038b9%2F</url>
    <content type="text"><![CDATA[MySQL：我就是个工具人…![kevin-ku-364843-unsplash](https://image.alessa0.cn/024045.jpg)MySQL 安装配置MySQL# 安装mysql 仅在Master安装mariadb： 开源版MySQLyum -y install mariadb-server mariadbrpm -q mariadb mariadb-server设置开机启动：systemctl enable mariadbsystemctl daemon-reload开启mysql：systemctl start mariadb关闭mysql：systemctl stop mariadb重启mysql：systemctl restart mariadb查看mysql状态：systemctl status mariadb通过内置的安全脚本实现对数据库的安全保护mysql_secure_installation创建Hive账户登录root账户：mysql -uroot -p创建账户：CREATE USER ‘alessa0’@’%’ IDENTIFIED BY ‘{密码}’;设置mysql远程登录：GRANT ALL ON . TO ‘alessa0’@’%’;刷新权限：flush privileges;推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on YarnMySQL查询MySQL查询]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>环境搭建</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hadoop环境搭建</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 集群搭建(五)：Redis]]></title>
    <url>%2Fposts%2F9aff70cb%2F</url>
    <content type="text"><![CDATA[Redis：就是这么快！![shiro-hatori-258976-unsplash](https://image.alessa0.cn/163221.jpg)Redis 安装配置Redis# 安装rediscd /mnt/hgfs/Hadoopcp redis-5.0.4.tar.gz /usr/local/src/cd /usr/local/src/tar zxvf redis-5.0.4.tar.gzrm -rf redis-5.0.4.tar.gz待续……推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on Yarn针对Redis默认端口的挖矿脚本分析Python 实现 Redis 异步客户端]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>环境搭建</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hadoop环境搭建</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 集群搭建(四)：ZooKeeper]]></title>
    <url>%2Fposts%2F9c5427b6%2F</url>
    <content type="text"><![CDATA[ZooKeeper：你们都安分点…![daiga-ellaby-354470-unsplash](https://image.alessa0.cn/051510.jpg)ZooKeeper 安装配置Zookeeper# 安装Zookeepercd /mnt/hgfs/Hadoopcp zookeeper-3.4.14.tar.gz /usr/local/src/cd /usr/local/srctar zxvf zookeeper-3.4.14.tar.gzrm -rf zookeeper-3.4.14.tar.gz配置Zookeeper环境变量：vim ~/.bashrc1234# 添加如下信息# SET ZOOKEEPER PATH export ZOOKEEPER_HOME=/usr/local/src/zookeeper-3.4.14 export PATH=$PATH:$ZOOKEEPER_HOME/binsource ~/.bashrc修改Zookeeper配置cd zookeeper-3.4.14创建临时目录/日志目录：mkdir datamkdir logs配置文件：cd confcp zoo_sample.cfg zoo.cfgvim zoo.cfg123456# 添加如下信息dataDir=/usr/local/src/zookeeper-3.4.14/data dataLogDir=/usr/local/src/zookeeper-3.4.14/logs server.1=master:2888:3888 server.2=slave1:2888:3888 server.3=slave2:2888:3888分别添加唯一标识ID：仅在Masterecho “1” &gt; /usr/local/src/zookeeper-3.4.14/data/myid仅在Slava1echo “2” &gt; /usr/local/src/zookeeper-3.4.14/data/myid仅在Slava2echo “3” &gt; /usr/local/src/zookeeper-3.4.14/data/myid启动Zookeeper启动服务：zkServer.sh start查看运行状态：zkServer.sh statusjps关闭服务：zkServer.sh stop推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on Yarn]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>环境搭建</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hadoop环境搭建</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 集群搭建(三)：Hadoop]]></title>
    <url>%2Fposts%2F6617c8b9%2F</url>
    <content type="text"><![CDATA[Hadoop：我好了你们再上![664px-Hadoop_logo.svg](https://image.alessa0.cn/163232.jpeg)Hadoop 安装配置HadoopPs：除非特别指出仅在Master，则在所有节点配置 # 安装Hadoopcd /mnt/hgfs/Hadoopcp hadoop-2.8.5.tar.gz /usr/local/src/cd /usr/local/srctar zxvf hadoop-2.8.5.tar.gzrm -rf hadoop-2.8.5.tar.gz配置Hadoop环境变量：vim ~/.bashrc12345# 添加如下信息# SET HADOOP PATH export HADOOP_HOME=/usr/local/src/hadoop-2.8.5 export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbinsource ~/.bashrcHadoop配置文件cd hadoop-2.8.5创建临时目录和文件目录：mkdir -p /usr/local/src/hadoop-2.8.5/dfs/namemkdir -p /usr/local/src/hadoop-2.8.5/dfs/datamkdir -p /usr/local/src/hadoop-2.8.5/tmp/dfscd etc/hadoop仅在Client配置(若无可跳过此步骤)：vim core-site.xml123456789# 添加如下信息&lt;configuration&gt; &lt;property&gt; &lt;!-- 指定Hadoop所使用的文件系统Schema --&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;mv mapred-site.xml.template mapred-site.xmlvim mapred-site.xml123456789# 添加如下信息&lt;configuration&gt; &lt;property&gt; &lt;!-- 指定MapReduce程序运行在Yarn上 --&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;vim yarn-site.xml123456789# 添加如下信息&lt;configuration&gt; &lt;property&gt; &lt;!-- 指定ResourceManager地址 --&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;修改Hadoop配置文件：vim hadoop-env.sh12# 添加如下信息export JAVA_HOME=/usr/local/src/jdk1.8.0_212vim yarn-env.sh12# 添加如下信息export JAVA_HOME=/usr/local/src/jdk1.8.0_212vim slaves123# 添加如下信息slave1slave2vim core-site.xml1234567891011121314# 添加如下信息&lt;configuration&gt; &lt;property&gt; &lt;!-- 指定Hadoop所使用的文件系统Schema --&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定HDFS本地临时存放目录 --&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/src/hadoop-2.8.5/tmp&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;vim hdfs-site.xml123456789101112131415161718192021222324# 添加如下信息&lt;configuration&gt; &lt;property&gt; &lt;!-- 指定SecondaryNamenode端口地址 --&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:9001&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定HDFS本地Namenode存放目录 --&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/src/hadoop-2.8.5/dfs.name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定HDFS本地Datanode存放目录 --&gt; &lt;name&gt;dfs.datanode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/src/hadoop-2.8.5/dfs.data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- HDFS副本数量(小于等于从节点的数量) --&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;vim mapred-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 添加如下信息&lt;configuration&gt; &lt;property&gt; &lt;!-- 指定MapReduce程序运行在Yarn上 --&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置JHS --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master:19888&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.staging-dir&lt;/name&gt; &lt;value&gt;/usr/local/src/hadoop-2.8.5/tmp/hadoop-yarn/staging&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt; &lt;value&gt;$&#123;yarn.app.mapreduce.am.staging-dir&#125;/history/done_intermediate&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt; &lt;value&gt;$&#123;yarn.app.mapreduce.am.staging-dir&#125;/history/done&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.cleaner.enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.cleaner.interval-ms&lt;/name&gt; &lt;value&gt;86400000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.max-age-ms&lt;/name&gt; &lt;value&gt;604800000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.move.interval-ms&lt;/name&gt; &lt;value&gt;180000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;vim yarn-site.xml1234567891011121314151617181920212223242526272829303132333435# 添加如下信息&lt;configuration&gt; &lt;property&gt; &lt;!-- 指定reducer获取数据的方式--&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定reducer获取数据所需的类--&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定ResourceManager地址 --&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8035&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:8088&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;启动集群仅在Master初始化NameNode：hadoop namenode -format启动Hadoop集群：start-dfs.shstart-yarn.shWEB监控页面：HDFS：http://ip:50070YARN：http://ip:8088推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on Yarn]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>环境搭建</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hadoop环境搭建</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 集群搭建(二)：语言环境]]></title>
    <url>%2Fposts%2F18e4e892%2F</url>
    <content type="text"><![CDATA[Java, Python, Scala![maximilian-weisbecker-544039-unsplash](https://image.alessa0.cn/143320.jpg)人生苦短Ps：安装包均存放于vmware共享文件夹Hadoop中JavaAnaconda3Scala# 安装JDKcd /mnt/hgfs/Hadoopcp jdk-8u212-linux-x64.tar.gz /usr/local/src/cd /usr/local/src/tar zxvf jdk-8u212-linux-x64.tar.gzrm -rf jdk-8u212-linux-x64.tar.gz配置JDK环境变量：vim ~/.bashrc12345# 添加如下信息# SET JAVA PATH export JAVA_HOME=/usr/local/src/jdk1.8.0_212 export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib export PATH=$PATH:$JAVA_HOME/binsource ~/.bashrc安装anaconda3cd /mnt/hgfs/Hadoopcp Miniconda3-latest-Linux-x86_64.sh /usr/local/src/cd /usr/local/srcsudo yum -y install bzip2sh Miniconda3-latest-Linux-x86_64.shrm -rf Miniconda3-latest-Linux-x86_64.sh配置环境变量：source ~/.bashrc更新conda环境：conda update —all安装Scalacd /mnt/hgfs/Hadoopcp scala-2.11.12.tgz /usr/local/src/cd /usr/local/src/tar zxvf scala-2.11.12.tgzrm -rf scala-2.11.12.tgz配置Scala环境变量：vim ~/.bashrc1234# 添加如下信息# SET SCALA PATH export SCALA_HOME=/usr/local/src/scala-2.11.12 export PATH=$PATH:$SCALA_HOME/binsource ~/.bashrc推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on YarnPython-OpenCV学习笔记——像素读取和写入Python-OpenCV学习笔记——图片写入及图片质量]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>环境搭建</category>
        <category>语言</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hadoop环境搭建</tag>
        <tag>Java</tag>
        <tag>Python</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 集群搭建(一)：节点配置]]></title>
    <url>%2Fposts%2Fb7d573f0%2F</url>
    <content type="text"><![CDATA[虚拟机节点配置![arian-darvishi-1210156-unsplash](https://image.alessa0.cn/131223.jpg)Hello, BigData集群环境系统：CentOS-7-x86_64-Minimal-1810.isoMaster 192.168.69.101Slave1 192.168.69.102Slave2 192.168.69.103修改IP地址：vim /etc/sysconfig/network-scripts/ifcfg-ens33Master12345# 修改如下信息IPADDR=192.168.69.101 NETMASK=255.255.255.0 GATEWAY=192.168.69.2 DNS1=119.29.29.29仅在Slave112345# 修改如下信息IPADDR=192.168.69.102 NETMASK=255.255.255.0 GATEWAY=192.168.69.2 DNS1=119.29.29.29仅在Slave212345# 修改如下信息IPADDR=192.168.69.103 NETMASK=255.255.255.0 GATEWAY=192.168.69.2 DNS1=119.29.29.29关闭系统防火墙及内核防火墙永久关闭内核防火墙：yum -y install vimvim /etc/selinux/config12# 修改如下信息SELINUX=disabled停止firewall：systemctl stop firewalld.service禁止firewall开机启动：systemctl disable firewalld.service修改主机文件修改主机名：仅在Masterhostnamectl set-hostname master仅在Slave1hostnamectl set-hostname slave1仅在Slave2hostnamectl set-hostname slave2修改hosts文件：vim /etc/hosts123456# 添加如下信息192.168.69.101 master 192.168.69.102 slave1 192.168.69.103 slave2 # 本地(可不加)192.168.69.1 clientSSH互信配置生成密钥对（公钥和私钥）–三次回车生成密钥：ssh-keygen -t rsa -P ‘’追加：cat ~/.ssh/id_rsa.pub &gt; ~/.ssh/authorized_keyschmod g-w ~chmod 700 ~/.sshchmod 600 ~/.ssh/authorized_keys追加密钥到Master：ssh [ 主机名 ] cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys复制密钥到从节点：scp /.ssh/authorized_keys [ 主机名 ]:/.ssh/authorized_keys设置vmware共享文件夹cd /mnt/hgfs/ 发现没有文件，解决如下安装工具：yum -y install open-vm-tools gcc gcc-c++ automake make kernel-devel git终端中输入如下命令：git clone https://github.com/rasa/vmware-tools-patches.gitcd vmware-tools-patches`./patched-open-vm-tools.sh查看分享目录：vmware-hgfsclientsu临时挂载分享目录：mount.vmhgfs .host:/ /mnt/hgfs/永久挂载分享目录：vmware-config-tools.pl -d –clobber-kernel-modules=vmhgfs#使用如下方法挂载有的会出错：vim /etc/fstab12# 末尾添加如下信息.host:/ /mnt/hgfs fuse.vmhgfs-fuse allow_other 0 0删除工具包cd ~rm -rf ~/vmware-tools-patchesreboot修改src权限cd /usr/localsudo chown -R alessa0:alessa0 srcsudo chown -R alessa0:alessa0 bin更改源为阿里云：cd /etc/yum.repos.d/mv CentOS-Base.repo CentOS-Base.repo.bakwget http://mirrors.aliyun.com/repo/Centos-7.repowget http://mirrors.163.com/.help/CentOS7-Base-163.repoyum clean allyum makecache安装网络工具包和基础工具包：sudo yum -y install net-tools checkpolicy gcc dkms foomatic openssh-server bash-completion psmiscHadoop集群组件列表组件masterslave1slave2jdk1.8.0_212√√√miniconda3√√√scala-2.11.12√√√hadoop-2.8.5√√√redis-5.0.4√zookeeper-3.4.14√√√spark-2.3.3-bin-hadoop2.7√√√mariadb/mariadb-server√apache-hive-2.3.4-bin√hbase-1.3.3√√√thrift-0.12.0√apache-flume-1.9.0-bin√√√kafka_2.11-2.2.0√√√apache-storm-1.2.2√√√Storm-on-Yarn√flink-1.8.0√√√推荐文章:Hadoop 集群搭建(八)：SparkHadoop 集群搭建(十四)：Storm on Yarn]]></content>
      <categories>
        <category>技术</category>
        <category>大数据</category>
        <category>环境搭建</category>
        <category>CentOS 7</category>
      </categories>
      <tags>
        <tag>BigData</tag>
        <tag>Hadoop环境搭建</tag>
        <tag>Vmware</tag>
      </tags>
  </entry>
</search>
